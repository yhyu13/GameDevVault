<!DOCTYPE html>
<!-- saved from url=(0125)https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding -->
<html lang="en" dir="ltr" xmlns:og="http://ogp.me/ns#" xmlns:article="http://ogp.me/ns/article#" xmlns:book="http://ogp.me/ns/book#" xmlns:profile="http://ogp.me/ns/profile#" xmlns:video="http://ogp.me/ns/video#" xmlns:product="http://ogp.me/ns/product#" class="js csstransforms csstransforms3d csstransitions"><head profile="http://www.w3.org/1999/xhtml/vocab"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/626218844182568" async=""></script><script async="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fbevents.js.download"></script><script type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/6f2048d7bc"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/nr-1169.min.js.download"></script><script type="text/javascript">(window.NREUM||(NREUM={})).loader_config={licenseKey:"6f2048d7bc",applicationID:"341156206"};window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var i=n[t]={exports:{}};e[t][0].call(i.exports,function(n){var i=e[t][1][n];return r(i||n)},i,i.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var i=0;i<t.length;i++)r(t[i]);return r}({1:[function(e,n,t){function r(){}function i(e,n,t){return function(){return o(e,[u.now()].concat(f(arguments)),n?null:this,t),n?void 0:this}}var o=e("handle"),a=e(4),f=e(5),c=e("ee").get("tracer"),u=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],l="api-",d=l+"ixn-";a(p,function(e,n){s[n]=i(l+n,!0,"api")}),s.addPageAction=i(l+"addPageAction",!0),s.setCurrentRouteName=i(l+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,i="function"==typeof n;return o(d+"tracer",[u.now(),e,t],r),function(){if(c.emit((i?"":"no-")+"fn-start",[u.now(),r,i],t),i)try{return n.apply(this,arguments)}catch(e){throw c.emit("fn-err",[arguments,this,e],t),e}finally{c.emit("fn-end",[u.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=i(d+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),o("err",[e,u.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){var t=e.getEntries();t.forEach(function(e){"first-paint"===e.name?c("timing",["fp",Math.floor(e.startTime)]):"first-contentful-paint"===e.name&&c("timing",["fcp",Math.floor(e.startTime)])})}function i(e,n){var t=e.getEntries();t.length>0&&c("lcp",[t[t.length-1]])}function o(e){if(e instanceof s&&!l){var n,t=Math.round(e.timeStamp);n=t>1e12?Date.now()-t:u.now()-t,l=!0,c("timing",["fi",t,{type:e.type,fid:n}])}}if(!("init"in NREUM&&"page_view_timing"in NREUM.init&&"enabled"in NREUM.init.page_view_timing&&NREUM.init.page_view_timing.enabled===!1)){var a,f,c=e("handle"),u=e("loader"),s=NREUM.o.EV;if("PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){a=new PerformanceObserver(r),f=new PerformanceObserver(i);try{a.observe({entryTypes:["paint"]}),f.observe({entryTypes:["largest-contentful-paint"]})}catch(p){}}if("addEventListener"in document){var l=!1,d=["click","keydown","mousedown","pointerdown","touchstart"];d.forEach(function(e){document.addEventListener(e,o,!1)})}}},{}],3:[function(e,n,t){function r(e,n){if(!i)return!1;if(e!==i)return!1;if(!n)return!0;if(!o)return!1;for(var t=o.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var i=null,o=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var f=navigator.userAgent,c=f.match(a);c&&f.indexOf("Chrome")===-1&&f.indexOf("Chromium")===-1&&(i="Safari",o=c[1])}n.exports={agent:i,version:o,match:r}},{}],4:[function(e,n,t){function r(e,n){var t=[],r="",o=0;for(r in e)i.call(e,r)&&(t[o]=n(r,e[r]),o+=1);return t}var i=Object.prototype.hasOwnProperty;n.exports=r},{}],5:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,i=t-n||0,o=Array(i<0?0:i);++r<i;)o[r]=e[n+r];return o}n.exports=r},{}],6:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function i(e){function n(e){return e&&e instanceof r?e:e?c(e,f,o):o()}function t(t,r,i,o){if(!l.aborted||o){e&&e(t,r,i);for(var a=n(i),f=v(t),c=f.length,u=0;u<c;u++)f[u].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function d(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||i(t)}function w(e,n){u(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:d,addEventListener:d,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function o(){return new r}function a(){(s.api||s.feature)&&(l.aborted=!0,s=l.backlog={})}var f="nr@context",c=e("gos"),u=e(4),s={},p={},l=n.exports=i();l.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(i.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(o){}return e[n]=r,r}var i=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){i.buffer([e],r),i.emit(e,n,t)}var i=e("ee").get("handle");n.exports=r,r.ee=i},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,o,function(){return i++})}var i=1,o="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!x++){var e=E.info=NREUM.info,n=d.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();u(y,function(n,t){e[n]||(e[n]=t)}),c("mark",["onload",a()+E.offset],null,"api");var t=d.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function i(){"complete"===d.readyState&&o()}function o(){c("mark",["domContent",a()+E.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(f=Math.max((new Date).getTime(),f))-E.offset}var f=(new Date).getTime(),c=e("handle"),u=e(4),s=e("ee"),p=e(3),l=window,d=l.document,m="addEventListener",v="attachEvent",g=l.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:l.setImmediate,CT:clearTimeout,XHR:g,REQ:l.Request,EV:l.Event,PR:l.Promise,MO:l.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1169.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),E=n.exports={offset:f,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),e(2),d[m]?(d[m]("DOMContentLoaded",o,!1),l[m]("load",r,!1)):(d[v]("onreadystatechange",i),l[v]("onload",r)),c("mark",["firstbyte",f],null,"api");var x=0,O=e(6)},{}],"wrap-function":[function(e,n,t){function r(e){return!(e&&e instanceof Function&&e.apply&&!e[a])}var i=e("ee"),o=e(5),a="nr@original",f=Object.prototype.hasOwnProperty,c=!1;n.exports=function(e,n){function t(e,n,t,i){function nrWrapper(){var r,a,f,c;try{a=this,r=o(arguments),f="function"==typeof t?t(r,a):t||{}}catch(u){l([u,"",[r,a,i],f])}s(n+"start",[r,a,i],f);try{return c=e.apply(a,r)}catch(p){throw s(n+"err",[r,a,p],f),p}finally{s(n+"end",[r,a,c],f)}}return r(e)?e:(n||(n=""),nrWrapper[a]=e,p(e,nrWrapper),nrWrapper)}function u(e,n,i,o){i||(i="");var a,f,c,u="-"===i.charAt(0);for(c=0;c<n.length;c++)f=n[c],a=e[f],r(a)||(e[f]=t(a,u?f+i:i,o,f))}function s(t,r,i){if(!c||n){var o=c;c=!0;try{e.emit(t,r,i,n)}catch(a){l([a,t,r,i])}c=o}}function p(e,n){if(Object.defineProperty&&Object.keys)try{var t=Object.keys(e);return t.forEach(function(t){Object.defineProperty(n,t,{get:function(){return e[t]},set:function(n){return e[t]=n,n}})}),n}catch(r){l([r])}for(var i in e)f.call(e,i)&&(n[i]=e[i]);return n}function l(n){try{e.emit("internal-error",n)}catch(t){}}return e||(e=i),t.inPlace=u,t.flag=a,t}},{}]},{},["loader"]);</script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/auth0.min.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/dz-auth.js.download"></script>
<link rel="shortcut icon" href="https://developer.nvidia.com/sites/all/themes/devzone_base/favicon.ico" type="image/vnd.microsoft.icon">
<link rel="alternate" type="application/rss+xml" title="RSS - Chapter 14. Perspective Shadow Maps: Care and Feeding" href="https://developer.nvidia.com/taxonomy/term/1318/feed">
<meta name="description" content="Chapter 14. Perspective Shadow Maps: Care and Feeding  Simon Kozlov  SoftLab-NSK  14.1 Introduction  Shadow generation has always been a big problem in real-time 3D graphics. Determining whether a point is in shadow is not a trivial operation for modern GPUs, particularly because GPUs work in terms of rasterizing polygons instead of ray  tracing.  Today&#39;s shadows should be">
<meta name="generator" content="Drupal 7 (http://drupal.org)">
<link rel="canonical" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding">
<link rel="shortlink" href="https://developer.nvidia.com/taxonomy/term/1318">
<meta property="og:site_name" content="NVIDIA Developer">
<meta property="og:type" content="article">
<meta property="og:url" content="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding">
<meta property="og:title" content="Chapter 14. Perspective Shadow Maps: Care and Feeding">
<meta property="og:description" content="Chapter 14. Perspective Shadow Maps: Care and Feeding  Simon Kozlov  SoftLab-NSK  14.1 Introduction  Shadow generation has always been a big problem in real-time 3D graphics. Determining whether a point is in shadow is not a trivial operation for modern GPUs, particularly because GPUs work in terms of rasterizing polygons instead of ray  tracing.  Today&#39;s shadows should be completely dynamic. Almost every object in the scene should cast and receive shadows, there should be self-shadowing, and every object should have soft shadows. Only two algorithms can satisfy these requirements:  shadow volumes (or stencil shadows) and shadow mapping.  The difference between the algorithms for shadow volumes and shadow mapping comes down to object space versus image space:     Object-space shadow algorithms, such as shadow volumes, work by creating a polygonal structure that represents the shadow occluders, which means that we always have pixel-accurate, but hard, shadows. This method cannot deal with   objects that have no polygonal structure, such as alpha-test-modified geometry or displacement mapped geometry. Also, drawing shadow volumes requires a lot of fill rate, which makes it difficult to use them for every object in a dense scene,   especially when there are multiple lights.   Image-space shadow algorithms deal with any object modification (if we can render an object, we&#39;ll have shadows), but they suffer from aliasing. Aliasing often occurs in large scenes with wide or omnidirectional light sources.   The problem is that the projection transform used in shadow mapping changes the screen size of the shadow map texels so that texels near the camera become very large. As a result, we have to use enormous shadow maps (four times the screen   resolution or larger) to achieve good quality. Still, shadow maps are much faster than shadow volumes in complex scenes.    Perspective shadow maps (PSMs), presented at SIGGRAPH 2002 by Stamminger and Drettakis (2002), try to eliminate aliasing in shadow maps by using them in post-projective space, where all nearby objects become larger than  farther ones. Unfortunately, it&#39;s difficult to use the original algorithm because it works well only in certain cases.  The most significant problems of the presented PSM algorithm are these three:     To hold all potential shadow casters inside the virtual camera frustum, &quot;virtual cameras&quot; are used when the light source is behind the camera. This results in poor shadow quality.   The shadow quality depends heavily on the light position in camera space.   Biasing problems weren&#39;t discussed in the original paper. Bias is a problem with PSMs because the texel area is distributed in a nonuniform way, which means that bias cannot be a constant anymore and should depend on the texel   position.    Each of these problems is discussed in the next section. This chapter focuses on directional lights (because they have bigger aliasing problems), but all the ideas and algorithms can easily be applied to other types of light source (details  are provided, where appropriate). In addition, we discuss tricks for increasing the quality of the shadow map by filtering and blurring the picture.  In general, this chapter describes techniques and methods that can increase the effectiveness of using PSMs. However, most of these ideas still should be adapted to your particular needs.  14.2 Problems with the PSM Algorithm  14.2.1 Virtual Cameras  First, let&#39;s look at the essence of this problem. The usual projective transform moves objects behind the camera to the other side of the infinity plane in post-projective space. However, if the light source is behind the camera too, these  objects are potential shadow casters and should be drawn into the shadow map.  In the perspective transform in Figure 14-1, the order of the points on the ray changes. The authors of the original PSM paper propose &quot;virtually&quot; sliding the view camera back to hold potential shadow casters in the viewing frustum, as  shown in Figure 14-2, so that we can use PSMs the normal way.       Figure 14-1 An Object Behind the Camera in Post-Projective Space         Figure 14-2 Using a Virtual Camera    Virtual Camera Issues  In practice, however, using the virtual camera leads to poor shadow quality. The &quot;virtual&quot; shift greatly decreases the resolution of the effective shadow map, so that objects near the real camera become smaller, and we end up with a lot of  unused space in the shadow map. In addition, we may have to move the camera back significantly for large shadow-casting objects behind the camera. Figure 14-3 shows how dramatically the quality changes, even with a small shift.       Figure 14-3 The Effect of the Virtual Shift on Shadow Quality    Another problem is minimizing the actual &quot;slideback distance,&quot; which maximizes image quality. This requires us to analyze the scene, find potential shadow casters, and so on. Of course, we could use bounding volumes, scene hierarchical  organizations, and similar techniques, but they would be a significant CPU hit. Moreover, we&#39;ll always have abrupt changes in shadow quality when an object stops being a potential shadow caster. In this case, the slideback distance instantly  changes, causing the shadow quality to change suddenly as well.  A Solution for Virtual Camera Issues  We propose a solution to this virtual camera problem: Use a special projection transform for the light matrix. In fact, post-projective space allows some projection tricks that can&#39;t be done in the usual world space. It turns out that we  can build a special projection matrix that can see &quot;farther than infinity.&quot;  Let&#39;s look at a post-projective space formed by an original (nonvirtual) camera with a directional &quot;inverse&quot; light source and with objects behind the view camera, as shown in Figure 14-4.       Figure 14-4 Post-Projective Space with an Inverse Light Source    A drawback to this solution is that the ray should (but doesn&#39;t) come out from the light source, catch point 1, go to minus infinity, then pass on to plus infinity and return to the light source, capturing information at points 2, 3, and 4.  Fortunately, there is a projection matrix that matches this &quot;impossible&quot; ray, where we can set the near plane to a negative value and the far plane to a positive value. See Figure 14-5.       Figure 14-5 An Inverse Projection Matrix    In the simplest case,  where a is small enough to fit the entire unit cube. Then we build this inverse projection as the usual projection matrix, as shown here, where matrices are written in a row-major style:  So the formula for the resulting transformed z coordinates, which go into a shadow map, is:  Z psm (-a) = 0 , and if we keep decreasing the z value to minus infinity, Z psm tends to ½. The same Z psm =  ½ corresponds to plus infinity, and moving from plus infinity to the far plane increases Z psm to 1 at the far plane. This is why the ray hits all points in the correct order and why there&#39;s no need to use  &quot;virtual slides&quot; for creating post-projective space.  This trick works only in post-projective space because normally all points behind the infinity plane have w &lt; 0, so they cannot be rasterized. But for another projection transformation caused by a light camera, these points are  located behind the camera, so the w coordinate is inverted again and becomes positive.  By using this inverse projection matrix, we don&#39;t have to use virtual cameras. As a result, we get much better shadow quality without any CPU scene analysis and the associated artifacts.  The only drawback to the inverse projection matrix is that we need a better shadow map depth-value precision, because we use big z-value ranges. However, 24-bit fixed-point depth values are enough for reasonable cases.  Virtual cameras still could be useful, though, because the shadow quality depends on the location of the camera&#39;s near plane. The formula for post-projective z is:  As we can see, Q is very close to 1 and doesn&#39;t change significantly as long as Z n is much smaller than Z f , which is typical. That&#39;s why the near and far planes have to  be changed significantly to affect the Q value, which usually is not possible. At the same time, near-plane values highly influence the post-projective space. For example, for Z n = 1 meter (m), the first  meter in the world space after the near plane occupies half the unit cube in post-projective space. In this respect, if we change Z n to 2 m, we will effectively double the z-value resolution and increase  the shadow quality. That means that we should maximize the Z n value by any means.  The perfect method, proposed in the original PSM article, is to read back the depth buffer, scan through each pixel, and find the maximum possible Z n for each frame. Unfortunately, this method is quite  expensive: it requires reading back a large amount of video memory, causes an additional CPU/GPU stall, and doesn&#39;t work well with swizzled and hierarchical depth buffers. So we should use another (perhaps less accurate) method to find a  suitable near-plane value for PSM rendering.  Such other methods for finding a suitable near-plane value for PSM rendering could include various methods of CPU scene analysis:     A method based on rough bounding-volume computations (briefly described later in &quot;The Light Camera&quot;).   A collision-detection system to estimate the distance to the closest object.   Additional software scene rendering with low-polygon-count level-of-details, which could also be useful for occlusion culling.   Sophisticated analysis based on particular features of scene structure for the specific application. For example, when dealing with scenarios using a fixed camera path, you could precompute the near-plane values for every frame.    These methods try to increase the actual near-plane value, but we could also increase the value &quot;virtually.&quot; The idea is the same as with the old virtual cameras, but with one difference. When sliding the camera back, we increase  the near-plane value so that the near-plane quads of the original and virtual cameras remain on the same plane. See Figure 14-6.       Figure 14-6 Difference Between Virtual Cameras    When we slide the virtual camera back, we improve the z-values resolution. However, this makes the value distribution for x and y values worse for near objects, thus balancing shadow quality near and far from the  camera. Because of the very irregular z-value distribution in post-projective space and the large influence of the near-plane value, this balance could not be achieved without this &quot;virtual&quot; slideback. The usual problem of shadows  looking great near the camera but having poor quality on distant objects is the typical result of unbalanced shadow map texel area distribution.  14.2.2 The Light Camera  Another problem with PSMs is that the shadow quality relies on the relationship between the light and camera positions. With a vertical directional light, aliasing problems are completely removed, but when light is directed toward the  camera and is close to head-on, there is significant shadow map aliasing.  We&#39;re trying to hold the entire unit cube in a single shadow map texture, so we have to make the light&#39;s field of view as large as necessary to fit the entire cube. This in turn means that the objects close to the near plane won&#39;t receive  enough texture samples. See Figure 14-7.       Figure 14-7 The Light Camera with a Low Light Angle    The closer the light source is to the unit cube, the poorer the quality. As we know,  so for large outdoor scenes that have Z n = 1 and Z f = 4000, Q = 1.0002, which means that the light source is extremely close to the unit cube. The Zf  /Zn correlation is usually bigger than 50, which corresponds to Q = 1.02, which is close enough to create problems.  We&#39;ll always have problems fitting the entire unit cube into a single shadow map texture. Two solutions each tackle one part of the problem: Unit cube clipping targets the light camera only on the necessary part of the unit cube,  and the cube map approach uses multiple textures to store depth information.  Unit Cube Clipping  This optimization relies on the fact that we need shadow map information only on actual objects, and the volume occupied by these objects is usually much smaller than the whole view frustum volume (especially close to the far plane). That&#39;s  why if we tune the light camera to hold real objects only (not the entire unit cube), we&#39;ll receive better quality. Of course, we should tune the camera using a simplified scene structure, such as bounding volumes.  Cube clipping was mentioned in the original PSM article, but it took into account all objects in a scene, including shadow casters in the view frustum and potential shadow casters outside the frustum for constructing the virtual camera.  Because we don&#39;t need virtual cameras anymore, we can focus the light camera on shadow receivers only, which is more efficient. See Figure 14-8. Still, we should choose near and far clip-plane values for the light camera in  post-projective space to hold all shadow casters in the shadow map. But it doesn&#39;t influence shadow quality because it doesn&#39;t change the texel area distribution.       Figure 14-8 Focusing the Light Camera Based on the Bounding Volumes of Shadow Receivers    Because faraway parts of these bounding volumes contract greatly in post-projective space, the light camera&#39;s field of view doesn&#39;t become very large, even with light sources that are close to the rest of the scene.  In practice, we can use rough bounding volumes to retain sufficient quality—we just need to indicate generally which part of the scene we are interested in. In outdoor scenes, it&#39;s the approximate height of objects on the landscape;  in indoor scenes, it&#39;s a bounding volume of the current room, and so on.  We&#39;d like to formalize the algorithm of computing the light camera focused on shadow receivers in the scene after we build a set of bounding volumes roughly describing the scene. In fact, the light camera is given by position, direction, up  vector, and projection parameters, most of which are predefined:     We can&#39;t change the position: it&#39;s a light position in post-projective space and nothing else.   In practice, the up vector doesn&#39;t change quality significantly, so we can choose anything reasonable.   Projection parameters are entirely defined by the view matrix.    So the most interesting thing is choosing the light camera direction based on bounding volumes. The proposed algorithm is this:     Compute the vertex list of constructive solid geometry operation, where B i is the ith bounding volume, F is the frustum for every shadow caster bounding volume that we see in the current   frame, and all these operations are performed in a view camera space. Then transform all these vertices into post-projective space. After this step, we have all the points that the light camera should &quot;see.&quot; (By the way, we should find a   good near-plane value based on these points, because reading back the depth buffer isn&#39;t a good solution.)   Find a light camera. As we already know, this means finding the best light camera direction, because all other parameters are easily computed for a given direction. We propose approximating the optimal direction by the axis of the   minimal cone, centered in the light source and including all the points in the list. The algorithm that finds the optimal cone for a set of points works in linear time, and it is similar to an algorithm that finds the smallest bounding   sphere for a set of points in linear time (Gartner 1999).    In this way, we could find an optimal light camera in linear time depending on the bounding volume number, which isn&#39;t very large because we need only rough information about the scene structure.  This algorithm is efficient for direct lights in large outdoor scenes. The shadow quality is almost independent of the light angle and slightly decreases if light is directed toward the camera. Figure 14-9 shows the difference between using  unit cube clipping and not using it.       Figure 14-9 Unit Cube Clipping    Using Cube Maps  Though cube clipping is efficient in some cases, other times it&#39;s difficult to use. For example, we might have a densely filled unit cube (which is common), or we may not want to use bounding volumes at all. Plus, cube clipping does not  work with point lights.  A more general method is to use a cube map texture for shadow mapping. Most light sources become point lights in post-projective space, and it&#39;s natural to use cube maps for shadow mapping with point light sources. But in post-projective  space, things change slightly and we should use cube maps differently because we need to store information about the unit cube only.  The proposed solution is to use unit cube faces that are back facing, with respect to the light, as platforms for cube-map-face textures.  For a direct light source in post-projective space, the cube map looks like Figure 14-10.       Figure 14-10 Using a Cube Map for Direct Lights    The number of used cube map faces (ranging from three to five) depends on the position of the light. We use the maximum number of faces when the light is close to the rest of the scene and directed toward the camera, so additional texture  resources are necessary. For other types of light sources located outside the unit cube, the pictures will be similar.  For a point light located inside the unit cube, we should use all six cube map faces, but they&#39;re still focused on unit cube faces. See Figure 14-11.       Figure 14-11 Using Cube Maps with a Point Light    We could say we form a &quot;cube map with displaced center,&quot; which is similar to a normal cube map, but with a constant vector added to its texture coordinates. In other words, texture coordinates for cube maps are vertex positions in  post-projective space shifted by the light source position:     Texture coordinates = vertex position - light position    By choosing unit cube faces as the cube map platform, we distribute the texture area proportionally to the screen size and ensure that shadow quality doesn&#39;t depend on the light and camera positions. In fact, texel size in post-projective  space is in a guaranteed range, so its projection on the screen depends only on the plane it&#39;s projected onto. This projection doesn&#39;t stretch texels much, so the texel size on the screen is within guaranteed bounds also.  Because the vertex and pixel shaders are relatively short when rendering the shadow map, what matters is the pure fill rate for the back buffer and the depth shadow map buffer. So there&#39;s almost no difference between drawing a single shadow  map and drawing a cube map with the same total texture size (with good occlusion culling, though). The cube map approach has better quality with the same total texture size as a single texture. The difference is the cost of the render target  switch and the additional instructions to compute cube map texture coordinates in the vertex and pixel shaders.  Let&#39;s see how to compute these texture coordinates. First, consider the picture shown in Figure 14-12. The blue square is our unit cube, P is the light source point, and V is the point for which we&#39;re generating texture  coordinates. We render all six cube map faces in separate passes for the shadow map; the near plane for each pass is shown in green. They&#39;re forming another small cube, so Z 1 = Z n /Z  f is constant for every pass.       Figure 14-12 A Detailed Cube Map View in Post-Projective Space    Now we should compute texture coordinates and depth values to compare for the point V. This just means that we should move this point in the (V - P ) direction until we intersect the cube. Consider d  1, d 2, d 3, d 4, d 5, and d 6 (see the face numbers in Figure 14-12) as the distances from P to each cube map face.  The point on the cube we are looking for (which is also the cube map texture coordinate) is:  Compare the value in the texture against the result of the projective transform of the a value. Because we already divided it by the corresponding d value, thus effectively making Z f = 1 and  Z n = Z 1, all we have to do is apply that projective transform. Note that in the case of the inverse camera projection from Section 14.2.1, Z n = -Z  1, Z f = Z 1.  (All these calculations are made in OpenGL-like coordinates, where the unit cube is actually a unit cube. In Direct3D, the unit cube is half the size, because the z coordinate is in the [0..1] range.)  Listing 14-1 is an example of how the shader code might look.  Example 14-1. Shader Code for Computing Cube Map Texture Coordinates  // c[d1] = 1/d1, 1/d2, 1/d3, 0 // c[d2] = -1/d4, -1/d5, -1/d6, 0 // c[z] = Q, -Q * Zn, 0, 0 // c[P] = P // r[V] = V // cbmcoord - output cube map texture coordinates // depth - depth to compare with shadow map values //Per-vertex level sub r[VP], r[V], c[P] mul r1, r[VP], c[d1] mul r2, r[VP], c[d2] //Per-pixel level max r3, r1, r2 max r3.x, r3.x, r3.y max r3.x, r3.x, r3.z rcp r3.w, r3.x mad cbmcoord, r[VP], r3.w, c[P] rcp r3.x, r3.w mad depth, r3.x, c[z].x, c[z].y   Because depth textures cannot be cube maps, we could use color textures, packing depth values into the color channels. There are many ways to do this and many implementation-dependent tricks, but their description is out of the scope of  this chapter.  Another possibility is to emulate this cube map approach with multitexturing, in which every cube map face becomes an independent texture (depth textures are great in this case). We form several texture coordinate sets in the vertex shader  and multiply by the shadow results in the pixel shader. The tricky part is to manage these textures over the objects in the scene, because every object rarely needs all six faces.  14.2.3 Biasing  As we stated earlier, the constant bias that is typically used in uniform shadow maps cannot be used with PSMs because the z values and the texel area distributions vary greatly with different light positions and points in the  scene.  If you plan to use depth textures, try z slope–scaled bias for biasing. It&#39;s often enough to fix the artifacts, especially when very distant objects don&#39;t fall within the camera. However, some cards do not support depth  textures (in DirectX, depth textures are supported only by NVIDIA cards), and depth textures can&#39;t be a cube map. In these cases, you need a different, more general algorithm for calculating bias. Another difficulty is that it&#39;s hard to  emulate and tweak z slope–scaled bias because it requires additional data—such as the vertex coordinates of the current triangle—passed into the pixel shader, plus some calculations, which isn&#39;t robust at all.  Anyway, let&#39;s see why we can&#39;t use constant bias anymore. Consider these two cases: the light source is near the unit cube, and the light source is far from the unit cube. See Figure 14-13.       Figure 14-13 Light Close to and Far from the Unit Cube    The problem is that the Z f /Z n correlation, which determines the z-value distribution into a shadow map, varies a lot in these two cases. So the constant bias would mean a  totally different actual bias in world and post-projective space: The constant bias tuned to the first light position won&#39;t be correct for the second light, and vice versa. Meanwhile, Z f /Z  n changes a lot, because the light source could be close to the unit cube and could be distant (even at infinity), depending on the relative positions of the light and the camera in world space.  Even with a fixed light source position, sometimes we cannot find a suitable constant for the bias. The bias should depend on the point position—because the projective transform enlarges the near objects and shrinks the far  ones—so the bias should be smaller near the camera and bigger for distant objects. Figure 14-14 shows the typical artifacts of using a constant bias in this situation.       Figure 14-14 Artifacts with Constant Bias    In short, the proposed solution is to use biasing in world space (and not to analyze the results of the double-projection matrix) and then transform this world-space bias in post-projective space. The computed value depends on the double  projection, and it&#39;s correct for any light and camera position. These operations could be done easily in a vertex shader. Furthermore, this world-space bias value should be scaled by texel size in world space to deal with artifacts caused by  the distribution of nonuniform texel areas.     Pbiased = (P orig + L(a + bLtexel ))M,    where P orig is the original point position, L is the light vector direction in world space, Ltexel is the texel size in world space, M is the final shadow map matrix, and  a and b are bias coefficients.  The texel size in world space could be approximately computed with simple matrix calculations. First, transform the point into shadow map space, and then shift this point by the texel size without changing depth. Next, transform it back  into world space and square the length of the difference between this point and the original one. This gives us Ltexel :  and S x and S y are shadow map resolutions.  Obviously, we can build a single matrix that performs all the transformations (except multiplying the coordinates, of course):  where M&#39; includes transforming, shifting, transforming back, and subtracting.  This turns out to be a rather empirical solution, but it should still be tweaked for your particular needs. See Figure 14-15.       Figure 14-15 Bias Calculated in the Vertex Shader    The vertex shader code that performs these calculations might look like Listing 14-2.  Example 14-2. Calculating Bias in a Vertex Shader  def c0, a, b, 0 ,0 // Calculating Ltexel dp4 r1.x, v0, c[LtexelMatrix_0] dp4 r1.y, v0, c[LtexelMatrix_1] dp4 r1.z, v0, c[LtexelMatrix_2] dp4 r1.w, v0, c[LtexelMatrix_3] // Transforming homogeneous coordinates // (in fact, we often can skip this step) rcp r1.w, r1.w mul r1.xy, r1.w, r1.xy // Now r1.x is an Ltexel mad r1.x, r1.x, c0.x, c0.y dp3 r1.x, r1, r1 // Move vertex in world space mad r1, v0, c[Lightdir], r1.x // Transform vertex into post-projective space // (we need z and w only) dp4 r[out].z, r1, c[M_2] dp4 r[out].w, r1, c[M_3]   The r[out] register holds the result of the biasing: the depth value, and the corresponding w, that should be interpolated across the triangle. Note that this interpolation should be separate from the interpolation of  texture coordinates (x, y, and the corresponding w), because these w coordinates are different. This biased value could be used when comparing with the shadow map value, or during the actual shadow map rendering (the  shadow map holds biased values).  14.3 Tricks for Better Shadow Maps  The advantage of shadow mapping over shadow volumes is the potential to create a color gradient between &quot;shadowed&quot; and &quot;nonshadowed&quot; samples, thus simulating soft shadows. This shadow &quot;softness&quot; doesn&#39;t depend on distance from the occluder,  light source size, and so on, but it still works in world space. Blurring stencil shadows, on the other hand, is more difficult, although Assarsson et al. (2003) make significant progress.  This section covers methods of filtering and blurring shadow maps to create a fake shadow softness that has a constant range of blurring but still looks good.  14.3.1 Filtering  Most methods of shadow map filtering are based on the percentage-closer filtering (PCF) principle. The only difference among the methods is how the hardware lets us use it. NVIDIA depth textures perform PCF after comparison with  the depth value; on other hardware, we have to take several samples from the nearest texels and average their results (for true PCF). In general, the depth texture filtering is more efficient than the manual PCF technique with four samples.  (PCF needs about eight samples to produce comparable quality.) In addition, using depth texture filtering doesn&#39;t forbid PCF, so we can take several filtered samples to further increase shadow quality.  Using PCF with PSMs is no different from using it with standard shadow maps: samples from neighboring texels are used for filtering. On the GPU, this is achieved by shifting texture coordinates one texel in each direction. For a more  detailed discussion of PCF, see Chapter 11, &quot;Shadow Map Antialiasing.&quot;  The shader pseudocode for PCF with four samples looks like Listings 14-3 and 14-4.  These tricks improve shadow quality, but they do not hide serious aliasing problems. For example, if many screen pixels map to one shadow map texel, large stair-stepping artifacts will be visible, even if they are somewhat blurred. Figure  14-16 shows an aliased shadow without any filtering, and Figure 14-17 shows how PCF helps improve shadow quality but cannot completely remove aliasing artifacts.       Figure 14-16 Strong Aliasing         Figure 14-17 Filtered Aliasing    Example 14-3. Vertex Shader Pseudocode for PCF  def c0, sample1x, sample1Y, 0, 0 def c1, sample2x, sample2Y, 0, 0 def c2, sample3x, sample3Y, 0, 0 def c3, sample4x, sample4Y, 0, 0 // The simplest case: // def c0, 1 / shadowmapsizeX, 1 / shadowmapsizeY, 0, 0 // def c1, -1 / shadowmapsizeX, -1 / shadowmapsizeY, 0, 0 // def c2, -1 / shadowmapsizeX, 1/ shadowmapsizeY, 0, 0 // def c3, 1 / shadowmapsizeX, -1 / shadowmapsizeY, 0, 0 . . . // Point - vertex position in light space mad oT0, point.w, c0, point mad oT1, point.w, c1, point mad oT2, point.w, c2, point mad oT3, point.w, c3, point   Example 14-4. Pixel Shader Pseudocode for PCF  def c0, 0.25, 0.25, 0.25, 0.25 tex t0 tex t1 tex t2 tex t3 . . . // After depth comparison mul r0, t0, c0 mad r0, t1, c0, r0 mad r0, t2, c0, r0 mad r0, t3, c0, r0   14.3.2 Blurring  As we know from projective shadows, the best blurring results often come from rendering to a smaller resolution texture with a pixel shader blur, then feeding this resulting texture back through the blur pixel shader several times (known as  ping-pong rendering). Shadow mapping and projective shadows are similar techniques, so why can&#39;t we use this method? The answer: because the shadow map isn&#39;t a black-and-white picture; it&#39;s a collection of depth values, and &quot;blurring  a depth map&quot; doesn&#39;t make sense.  In fact, the proposal is to use the color part of the shadow map render (which comes almost for free) as projective texture for some objects. For example, assume that we have an outdoor landscape scene and we want a high-quality blurred  shadow on the ground because ground shadows are the most noticeable.     Before rendering the depth shadow map, clear the color buffer with 1. During the render, draw 0 into the color buffer for every object except the landscape; for the landscape, draw 1 in color. After the whole shadow map renders, we&#39;ll   have 1 where the landscape is nonshadowed and 0 where it&#39;s shadowed. See Figure 14-18.          Figure 14-18 The Original Color Part for a Small Test Scene        Blur the picture (the one in Figure 14-18) severely, using multiple passes with a simple blur pixel shader. For example, using a simple two-pass Gaussian blur gives good results. (You might want to adjust the blurring radius for distant   objects.) After this step, we&#39;ll have a high-quality blurred texture, as shown in Figure 14-19.          Figure 14-19 The Blurred Color Part for a Small Test Scene        While rendering the scene with shadows, render the landscape with the blurred texture instead of the shadow map, and render all other objects with the depth part of the shadow map. See Figure 14-20.          Figure 14-20 Applying Blurring to a Real Scene         The difference in quality is dramatic.  Of course, we can use this method not only with landscapes, but also with any object that does not need self-shadowing (such as floors, walls, ground planes, and so on). Fortunately, in these areas shadows are most noticeable and aliasing  problems are most evident. Because we have several color channels, we can blur shadows on several objects at the same time:     Using depth textures, the color buffer is completely free, so we can use all four channels for four objects.   For floating-point textures, one channel stores depth information, so we have three channels for blurring.   For fixed-point textures, depth is usually stored in the red and green channels, so we have only two free channels.    This way we&#39;ll have nice blurred shadows on the ground, floor, walls, and so on while retaining all other shadows (blurred with PCF) on other objects (with proper self-shadowing).  14.4 Results  The screenshots in Figures 14-21, 14-22, 14-23, and 14-24 were captured on the NVIDIA GeForce4 Ti 4600 in 1600x1200 screen resolution, with 100,000 to 500,000 visible polygons. All objects receive and cast shadows with real-time frame rates  (more than 30).       Figure 14-21         Figure 14-22         Figure 14-23         Figure 14-24    14.5 References  Assarsson, U., M. Dougherty, M. Mounier, and T. Akenine-Möller. 2003. &quot;An Optimized Soft Shadow Volume Algorithm with Real-Time Performance.&quot; In Proceedings of the SIGGRAPH/Eurographics  Workshop on Graphics Hardware 2003.  Gartner, Bernd. 1999. &quot;Smallest Enclosing Balls: Fast and Robust in C++.&quot; Web page. http://www.inf.ethz.ch/personal/gaertner/texts/own_work/esa99_final.pdf  Stamminger, Marc, and George Drettakis. 2002. &quot;Perspective Shadow Maps.&quot; In Proceedings of ACM SIGGRAPH 2002.  The author would like to thank Peter Popov for his many helpful and productive discussions.      Copyright  Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and Addison-Wesley was aware of a trademark claim, the designations have been  printed with initial capital letters or in all capitals.  The authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential  damages in connection with or arising out of the use of the information or programs contained herein.  The publisher offers discounts on this book when ordered in quantity for bulk purchases and special sales. For more information, please contact:        U.S. Corporate and Government Sales        (800) 382-3419        corpsales@pearsontechgroup.com  For sales outside of the U.S., please contact:        International Sales        international@pearsoned.com  Visit Addison-Wesley on the Web: www.awprofessional.com  Library of Congress Control Number: 2004100582  GeForce™ and NVIDIA Quadro® are trademarks or registered trademarks of NVIDIA Corporation.  RenderMan® is a registered trademark of Pixar Animation Studios.  &quot;Shadow Map Antialiasing&quot; © 2003 NVIDIA Corporation and Pixar Animation Studios.  &quot;Cinematic Lighting&quot; © 2003 Pixar Animation Studios.  Dawn images © 2002 NVIDIA Corporation. Vulcan images © 2003 NVIDIA Corporation.  Copyright © 2004 by NVIDIA Corporation.  All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior consent of the  publisher. Printed in the United States of America. Published simultaneously in Canada.  For information on obtaining permission for use of material from this work, please submit a written request to:        Pearson Education, Inc.        Rights and Contracts Department        One Lake Street        Upper Saddle River, NJ 07458  Text printed on recycled and acid-free paper.  5 6 7 8 9 10 QWT 09 08 07  5th Printing September 2007">
  <title>Chapter 14. Perspective Shadow Maps: Care and Feeding | NVIDIA Developer</title>
  <link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_lQaZfjVpwP_oGNqdtWCSpJT1EMqXdMiU84ekLLxQnc4.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_tGTNtRAsNJmcVz4r7F6YCijbrQGDbBmziHSrRd1Stqc.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_VWGhZkkR4B4tMJA7PC_wov8dAxaI-MS03BCM3K8jjJA.css" media="screen">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_dDInFmDfqK3xzE6xbY2x7CyR2nlLxtXmEsFMHKgv2LI.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_QRimMrZK-Qv-u5bY8suH0j8mDYtzTPrd8f5XrKM55DE.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/dz-auth.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_dxTC2SnbUQpi6ay7fqSk9MkxtE4JRKtOHqpCvu7mKNQ.css" media="all">
<link type="text/css" rel="stylesheet" href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/css_SxHAMz_4JUt6eFE4j6IYCx5GGkc_ADTuXAqx-5-6g84.css" media="all">
  <!-- HTML5 element support for IE6-8 -->
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_rwJOw8atmox9XV3v8iLC0A-YAmKyx85XDT2dIASfdKg.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_9k9l6haqJ3dZ6codRtL8eMyNhiLgYLyVEkAGirRC8ZQ.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/mbox-contents-076a7d6590086f9601d839f4444d591eddf5d2d1.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_vZhZBBTWXiuPYTWQtb98ufaeYSZGeZ48UnolFs6nIm0.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_oRBVbrZV7FAqu356imAriqUbp3bcQkSf7vs-78D-bp8.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_n2qkjJI1hBcni1gf_zugKfjd6Iu-WtHjIjJlkHrUyH4.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/d3.v4.min.js.download"></script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_1WtY6Ynl17870CZaYD2mSEcdwTB8LfJ-olTySHbEXdo.js.download"></script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"devzone_base","theme_token":"e_o0TYJHg4UeLIurdBcGmP9ODNa4UH7tidVQYIuVbnA","js":{"0":1,"1":1,"sites\/all\/themes\/bootstrap\/js\/bootstrap.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/horizontal-charts-init.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/jquery\/1.10\/jquery.min.js":1,"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.core.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.widget.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.button.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.mouse.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.draggable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.position.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.resizable.min.js":1,"sites\/all\/modules\/contrib\/jquery_update\/replace\/ui\/ui\/minified\/jquery.ui.dialog.min.js":1,"\/\/assets.adobedtm.com\/b92787824f2e0e9b68dc2e993f9bd995339fe417\/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js":1,"sites\/all\/modules\/contrib\/codefilter\/codefilter.js":1,"sites\/all\/modules\/custom\/nvidia_quick_survey\/nvidia_quick_survey.js":1,"sites\/all\/libraries\/colorbox\/jquery.colorbox-min.js":1,"sites\/all\/modules\/contrib\/colorbox\/js\/colorbox.js":1,"sites\/all\/modules\/contrib\/hint\/hint.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.migrate.min.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.isotope.js":1,"sites\/all\/themes\/devzone_base\/js\/jquery.sidr.js":1,"sites\/all\/themes\/devzone_base\/js\/datatables.min.js":1,"sites\/all\/themes\/devzone_base\/js\/js.cookie-2.2.1.min.js":1,"sites\/all\/themes\/devzone_base\/js\/application.js":1,"sites\/all\/themes\/devzone_base\/js\/attrchange.js":1,"sites\/all\/themes\/devzone_base\/js\/scripts.js":1,"sites\/all\/themes\/devzone_base\/js\/adroll.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/affix.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/alert.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/button.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/carousel.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/collapse.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/dropdown.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/modal.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/tooltip.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/popover.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/scrollspy.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/tab.js":1,"sites\/all\/themes\/devzone_base\/bootstrap\/js\/transition.js":1,"https:\/\/d3js.org\/d3.v4.min.js":1,"sites\/all\/modules\/custom\/nvidia_tokens\/js\/visualize-d.js":1},"css":{"modules\/system\/system.base.css":1,"misc\/ui\/jquery.ui.core.css":1,"misc\/ui\/jquery.ui.theme.css":1,"misc\/ui\/jquery.ui.button.css":1,"misc\/ui\/jquery.ui.resizable.css":1,"misc\/ui\/jquery.ui.dialog.css":1,"sites\/all\/modules\/contrib\/codefilter\/codefilter.css":1,"sites\/all\/modules\/contrib\/date\/date_api\/date.css":1,"sites\/all\/modules\/contrib\/date\/date_popup\/themes\/datepicker.1.7.css":1,"sites\/all\/modules\/contrib\/date\/date_repeat_field\/date_repeat_field.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"sites\/all\/modules\/custom\/nvidia_quick_survey\/nvidia_quick_survey.css":1,"sites\/all\/modules\/contrib\/views\/css\/views.css":1,"sites\/all\/modules\/contrib\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/custom\/nvidia_tokens\/css\/nvidia-charts.css":1,"https:\/\/developer.nvidia.com\/register\/dz-auth.css?v=3.2.5c":1,"sites\/all\/modules\/contrib\/addtoany\/addtoany.css":1,"sites\/all\/themes\/bootstrap\/css\/overrides.css":1,"sites\/all\/themes\/devzone_base\/css\/application.css":1,"sites\/all\/themes\/devzone_base\/css\/datatables.min.css":1}},"colorbox":{"opacity":"0.85","current":"{current} of {total}","previous":"\u00ab Prev","next":"Next \u00bb","close":"Close","maxWidth":"98%","maxHeight":"98%","fixed":true,"mobiledetect":true,"mobiledevicewidth":"480px"},"urlIsAjaxTrusted":{"\/search":true},"bootstrap":{"anchorsFix":1,"anchorsSmoothScrolling":1,"formHasError":1,"popoverEnabled":1,"popoverOptions":{"animation":1,"html":0,"placement":"right","selector":"","trigger":"click","triggerAutoclose":1,"title":"","content":"","delay":0,"container":"body"},"tooltipEnabled":1,"tooltipOptions":{"animation":1,"html":0,"placement":"auto left","selector":"","trigger":"hover focus","delay":0,"container":"body"}}});</script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satellite-599197ca64746d2c22002eb4.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satellite-594a6ce764746d506700c2a3.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satellite-5991afef64746d6deb010d48.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satellite-5991b02f64746d63240018f2.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/s-code-contents-72887b31a9638f8b3cff7981a426c38c354f6412.js.download"></script><script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/satellite-5dca7e8b64746d0f34001a19.js.download"></script><script type="text/javascript" async="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/c6af8848c2687.js.download"></script><script async="true" type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/roundtrip.js.download"></script><script id="adroll_scr_exp" onerror="window.adroll_exp_list = [];" type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/index.js.download"></script><script type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/index.js(1).download"></script><script type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/OYZWQRQDRZCBLGX5QLLXRG"></script><script async="true" type="text/javascript" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/XDTCNQ3C2NF3DKOAMKYLNF"></script><div style="width: 1px; height: 1px; display: inline; position: absolute;"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(1)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(2)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(3)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(4)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(5)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(6)"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(7)"></div><div style="width: 1px; height: 1px; display: inline; position: absolute;"><img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(5)">
<img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(8)">
<img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(9)">
<img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(10)">
<img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(11)">
<img height="1" width="1" style="border-style:none;" alt="" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/out(12)">
</div></head>
<body class="html not-front not-logged-in no-sidebars page-taxonomy page-taxonomy-term page-taxonomy-term- page-taxonomy-term-1318 nimbus-is-editor" data-gr-c-s-loaded="true" style="">
  <div id="skip-link">
    <a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#main-content" class="element-invisible element-focusable">Skip to main content</a>
  </div>
    

  <!--Navbar-->
  <nav class="navbar navbar-inverse navbar-static-top" role="navigation" id="nvidia-dropdown">
    <div class="container">
      <div class="navbar-header">
        <button class="navbar-toggle" type="button"><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button>
        <div class="logo-header">
          <a class="navbar-brand first-logo" href="https://developer.nvidia.com/" title="Home">
            <img alt="Home" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/nvidia.png">
          </a>
          <a class="navbar-brand second-logo" href="https://developer.nvidia.com/" title="Home">
            <img alt="Home" src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/logo.png">
          </a>
        </div>
      </div>
      <div class="collapse navbar-collapse" id="navbar-collapse">

                  <ul class="menu nav navbar-nav primary"><li class="first expanded megamenu dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Solutions <span class="caret"></span></a><div class="dropdown-menu"><div class="container-fluid"><ul class="navbar-nav"><li class="first expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">AI and Deep Learning</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/deep-learning">Deep Learning</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/machine-learning">Machine Learning</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/tensorrt">Inference</a></li>
<li class="leaf"><a href="https://www.nvidia.com/en-us/deep-learning-ai/education" title="">Deep Learning Institute</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/Clara-Genomics">Genomics</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/gpu-cloud/" title="">GPU-optimized S/W (NGC)</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/embedded-computing">Autonomous Machines</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/embedded/develop/hardware">Hardware (Jetson)</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk" title="">Robotics</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">Video Analytics</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/drive">Autonomous Vehicles</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/drive/drive-agx">Hardware (DRIVE AGX)</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-hyperion" title="">Reference Architecture</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-software">Autonomous Vehicle Software</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">Data Center Simulation Platform</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/rtx">Graphics and Simulation</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/graphics-research-tools" title="">Graphics Research Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/video-processing">Video Processing</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/rtx/raytracing" title="">Ray Tracing</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/rtx/ngx" title="">AI for Graphics</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworks-visualfx-overview" title="">Real-time VFX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks">Virtual and Augmented Reality</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/physx-sdk">Simulation</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara">Medical Imaging</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nvidia-index">Scientific Visualization</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/design-visualization/solutions/quadro-display-desktop-management#Management">Display</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/hpc">High-performance Computing</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/language-solutions">Languages and APIs</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gpu-accelerated-libraries" title="">GPU Accelerated Libraries</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/openacc">OpenACC Programming Model</a></li>
</ul></li>
<li class="last expanded dropdown"><a href="https://developer.nvidia.com/tools-overview">Tools and Management</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/tools-overview" title="">Developer Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/what-is-designworks">Management Tools</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/tools-overview">Android and Tegra for Mobile</a></li>
</ul></li>
</ul></div></div></li>
<li class="expanded megamenu dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Platforms <span class="caret"></span></a><div class="dropdown-menu"><div class="container-fluid"><ul class="navbar-nav"><li class="first expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">CUDA-X AI</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/tensorrt">TensorRT</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cudnn">cuDNN</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nccl">NCCL</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cublas">cuBLAS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cusparse">cuSPARSE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/opticalflow-sdk">Optical Flow SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/DALI">DALI</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/transfer-learning-toolkit">Transfer Learning Toolkit</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/digits">DIGITS</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">CLARA</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/clara-guardian" title="">Clara Guardian</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara-medical-imaging" title="">Clara Imaging</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/clara-parabricks">Clara Parabricks</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">HPC</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/cuda-zone">CUDA Toolkit</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/openacc">OpenACC</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">DRIVE</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/drive/drive-agx">DRIVE AGX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-hyperion">DRIVE Hyperion</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">DRIVE Sim</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">DRIVE Constellation</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/data-center/dgx-systems/">DGX</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">RTX</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/optix">OptiX SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks/vrworks-audio" title="">Path-traced Audio (VRWorks) </a></li>
<li class="leaf"><a href="https://developer.nvidia.com/Vulkan">VKRay</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/mdl-sdk" title="">MDL SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vmaterials">vMaterials</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/physx-sdk">PhysX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/flex">Flex</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/opticalflow-sdk" title="">Optical Flow SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nvidia-video-codec-sdk">Video Codec SDK</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/gpudirectforvideo">GPUDirect for Video</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">ISAAC</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/embedded/develop/hardware">Jetson Developer Kits</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/embedded/jetpack">Jetpack</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac Robot Engine</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac Sim</a></li>
</ul></li>
<li class="last expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Metropolis</a><ul><li class="first last leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">DeepStream SDK</a></li>
</ul></li>
</ul></div></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Documentation <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://docs.nvidia.com/">Library</a></li>
<li class="leaf"><a href="https://raytracing-docs.nvidia.com/">Ray Tracing</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/cuda/index.html">CUDA Toolkit</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/gameworks/index.html">GameWorks</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/drive/index.html">DRIVE</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/ngc/index.html">NGC</a></li>
<li class="last leaf"><a href="https://docs.nvidia.com/isaac/index.html">Isaac</a></li>
</ul></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Downloads <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://developer.nvidia.com/cuda-toolkit" title="">CUDA Toolkit</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworksdownload#?tx=$gameworks,developer_tools" title="">Developer Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara">CLARA</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/downloads">DRIVE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/graphics-research-tools" title="">Graphics Research Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworksdownload">Gameworks</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/embedded/downloads">Jetson</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/deepstream-sdk">Metropolis</a></li>
</ul></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Resources <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://developer.nvidia.com/contact">Contact Us</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/developer-program">Developer Program</a></li>
<li class="leaf"><a href="https://courses.nvidia.com/">Deep Learning Institute</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/higher-education-and-research">Educators</a></li>
<li class="leaf"><a href="https://ngc.nvidia.com/">NGC</a></li>
<li class="leaf"><a href="https://resources.nvidia.com/l/events" title="">Event Recordings</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/open-source">Open Source</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/deep-learning-ai/startups/" title="">AI Startups</a></li>
</ul></div></li>
<li class="last expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Community <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://forums.developer.nvidia.com/" title="">Forums</a></li>
<li class="leaf"><a href="https://devblogs.nvidia.com/">Blog</a></li>
<li class="last leaf"><a href="https://news.developer.nvidia.com/">News</a></li>
</ul></div></li>
</ul>                <ul class="nav navbar-nav navbar-margin navbar-right navbar-margin-media login-nav">
                      <li class="search" id="search-top">
              <div class="search-form" id="search-top-form">
                <form class="gss form-search content-search" action="https://developer.nvidia.com/search" method="post" id="nvidia-site-search-form" accept-charset="UTF-8"><div><div class="input-group"><input placeholder="Search..." class="form-control form-text" type="text" id="edit-term" name="term" value="" size="15" maxlength="128"><span class="input-group-btn"><button type="submit" class="btn btn-default"><span class="icon glyphicon glyphicon-search" aria-hidden="true"></span>
</button></span></div><button class="btn element-invisible btn-primary form-submit" type="submit" id="edit-submit" name="op" value="Search">Search</button>
<input type="hidden" name="form_build_id" value="form-Pwhi73Mi4C3SFuYLq5o5BQB6iwi9QOLBZjdeeOUiabg">
<input type="hidden" name="form_id" value="nvidia_site_search_form">
</div></form>              </div>
            </li>
          
          
          
                      <li class="leaf last" id="dzauth_login_link"><a href="javascript:jQuery.sidr(&#39;close&#39;);javascript:showDzAuth(&#39;login&#39;);">Account</a></li>
          
        </ul>
      </div>
    </div>
  </nav>
      <nav class="navbar navbar-inverse second-navbar hidden-xs" role="navigation" id="nvidia-secondary-dropdown">
      <div class="container">
        <div class="collapse navbar-collapse">
          <ul class="menu nav navbar-nav secondary"><li class="first leaf"><a href="https://developer.nvidia.com/rtx" title="">RTX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworks" title="">GAMEWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/designworks" title="">DESIGNWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks" title="">VRWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/hpc" title="">HPC</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">METROPOLIS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive" title="">DRIVE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara" title="">CLARA</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/open-source" title="">OPEN SOURCE</a></li>
</ul>                  </div>
      </div>
    </nav>
  <div id="wrapper">
        
  <div id="content-background" class="white-background">
          <div id="console">
        <div class="container">
                            </div>
      </div>
    
    <div class="separator"></div>
    <div id="content" class="container">
              <ol class="breadcrumb hidden-xs"><li><a href="https://developer.nvidia.com/">Home</a></li><li><a href="https://developer.nvidia.com/gpugems/gpugems">GPUGems</a></li><li class="active"><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows">Part II: Lighting and Shadows</a></li></ol>      
                      <div class="separator"></div>
      
      
      <div class="row">

        
        <section class="col-sm-12">
          
                    <a id="main-content"></a>

            <div class="region region-content">
    <section id="block-system-main" class="block block-system clearfix">

      
  <div class="term-listing-heading">
<div id="book_switch">
  <a href="https://developer.nvidia.com/gpugems/gpugems" class="btn btn-primary">GPUGems</a><a href="https://developer.nvidia.com/gpugems/gpugems2" class="btn btn-primary">GPUGems2</a><a href="https://developer.nvidia.com/gpugems/gpugems3" class="btn btn-primary">GPUGems3</a></div>
<div id="book_page">
  <div class="row">
    <div class="col-md-8">
      <a href="http://developer.nvidia.com/gpugems"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/GPU_Gems_1.jpg" align="left" border="0" hspace="5"></a>        <h1><a href="http://developer.nvidia.com/gpugems">GPU Gems</a></h1> <b>GPU Gems</b> is now available, right here, online. You can <a href="http://www.informit.com/promotion/136275">purchase a beautifully printed version of       this book</a>, and others in the series, at a 30% discount courtesy of InformIT and Addison-Wesley.<br>       <br>       The CD content, including demos and content, is available on the <a target="_blank" href="https://http.download.nvidia.com/developer/GPU_Gems/CD_Image/Index.html">web</a> and for <a href="https://http.download.nvidia.com/developer/GPU_Gems/CD_Image/GPU_Gems_code.zip">download</a>.<br>       <br>       <br>       <hr>      <h1 class="docChapterTitle" data-parent="Part II: Lighting and Shadows">Chapter 14. Perspective Shadow Maps: Care and Feeding</h1>        <p><em>Simon Kozlov<br>       SoftLab-NSK</em></p>        <h2>14.1 Introduction</h2>        <p>Shadow generation has always been a big problem in real-time 3D graphics. Determining whether a point is in shadow is not a trivial operation for modern GPUs, particularly because GPUs work in terms of rasterizing polygons instead of ray       tracing.</p>        <p>Today's shadows should be completely dynamic. Almost every object in the scene should cast and receive shadows, there should be self-shadowing, and every object should have soft shadows. Only two algorithms can satisfy these requirements:       shadow volumes (or stencil shadows) and shadow mapping.</p>        <p>The difference between the algorithms for shadow volumes and shadow mapping comes down to <em>object space</em> versus <em>image space</em>:</p>        <ul>         <li> <em>Object-space shadow algorithms</em>, such as shadow volumes, work by creating a polygonal structure that represents the shadow occluders, which means that we always have pixel-accurate, but hard, shadows. This method cannot deal with         objects that have no polygonal structure, such as alpha-test-modified geometry or displacement mapped geometry. Also, drawing shadow volumes requires a lot of fill rate, which makes it difficult to use them for every object in a dense scene,         especially when there are multiple lights.</li>          <li> <em>Image-space shadow algorithms</em> deal with any object modification (if we can render an object, we'll have shadows), but they suffer from aliasing. Aliasing often occurs in large scenes with wide or omnidirectional light sources.         The problem is that the projection transform used in shadow mapping changes the screen size of the shadow map texels so that texels near the camera become very large. As a result, we have to use enormous shadow maps (four times the screen         resolution or larger) to achieve good quality. Still, shadow maps are much faster than shadow volumes in complex scenes.</li>       </ul>        <p><em>Perspective shadow maps</em> (PSMs), presented at SIGGRAPH 2002 by Stamminger and Drettakis (2002), try to eliminate aliasing in shadow maps by using them in <em>post-projective space</em>, where all nearby objects become larger than       farther ones. Unfortunately, it's difficult to use the original algorithm because it works well only in certain cases.</p>        <p>The most significant problems of the presented PSM algorithm are these three:</p>        <ul>         <li>To hold all potential shadow casters inside the virtual camera frustum, "virtual cameras" are used when the light source is behind the camera. This results in poor shadow quality.</li>          <li>The shadow quality depends heavily on the light position in camera space.</li>          <li>Biasing problems weren't discussed in the original paper. Bias is a problem with PSMs because the texel area is distributed in a nonuniform way, which means that bias cannot be a constant anymore and should depend on the texel         position.</li>       </ul>        <p>Each of these problems is discussed in the next section. This chapter focuses on directional lights (because they have bigger aliasing problems), but all the ideas and algorithms can easily be applied to other types of light source (details       are provided, where appropriate). In addition, we discuss tricks for increasing the quality of the shadow map by filtering and blurring the picture.</p>        <p>In general, this chapter describes techniques and methods that can increase the effectiveness of using PSMs. However, most of these ideas still should be adapted to your particular needs.</p>        <h2>14.2 Problems with the PSM Algorithm</h2>        <h4>14.2.1 Virtual Cameras</h4>        <p>First, let's look at the essence of this problem. The usual projective transform moves objects behind the camera to the other side of the infinity plane in post-projective space. However, if the light source is behind the camera too, these       objects are potential shadow casters and should be drawn into the shadow map.</p>        <p>In the perspective transform in Figure 14-1, the order of the points on the ray changes. The authors of the original PSM paper propose "virtually" sliding the view camera back to hold potential shadow casters in the viewing frustum, as       shown in Figure 14-2, so that we can use PSMs the normal way.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-01.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-01.jpg" alt="fig14-01.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-01.jpg">Figure 14-1</a> An Object Behind the Camera in Post-Projective Space</p>       </div>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-02.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-02.jpg" alt="fig14-02.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-02.jpg">Figure 14-2</a> Using a Virtual Camera</p>       </div>        <h4>Virtual Camera Issues</h4>        <p>In practice, however, using the virtual camera leads to poor shadow quality. The "virtual" shift greatly decreases the resolution of the effective shadow map, so that objects near the real camera become smaller, and we end up with a lot of       unused space in the shadow map. In addition, we may have to move the camera back significantly for large shadow-casting objects behind the camera. Figure 14-3 shows how dramatically the quality changes, even with a small shift.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-03a.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-03a.jpg" alt="fig14-03a.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-03a.jpg">Figure 14-3</a> The Effect of the Virtual Shift on Shadow Quality</p>       </div>        <p>Another problem is minimizing the actual "slideback distance," which maximizes image quality. This requires us to analyze the scene, find potential shadow casters, and so on. Of course, we could use bounding volumes, scene hierarchical       organizations, and similar techniques, but they would be a significant CPU hit. Moreover, we'll always have abrupt changes in shadow quality when an object stops being a potential shadow caster. In this case, the slideback distance instantly       changes, causing the shadow quality to change suddenly as well.</p>        <h4>A Solution for Virtual Camera Issues</h4>        <p>We propose a solution to this virtual camera problem: Use a special projection transform for the light matrix. In fact, post-projective space allows some projection tricks that can't be done in the usual world space. It turns out that we       can build a special projection matrix that can see "farther than infinity."</p>        <p>Let's look at a post-projective space formed by an original (nonvirtual) camera with a directional "inverse" light source and with objects behind the view camera, as shown in Figure 14-4.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-04.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-04.jpg" alt="fig14-04.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-04.jpg">Figure 14-4</a> Post-Projective Space with an Inverse Light Source</p>       </div>        <p>A drawback to this solution is that the ray should (but doesn't) come out from the light source, catch point 1, go to minus infinity, then pass on to plus infinity and return to the light source, capturing information at points 2, 3, and 4.       Fortunately, there is a projection matrix that matches this "impossible" ray, where we can set the near plane to a negative value and the far plane to a positive value. See Figure 14-5.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-05.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-05.jpg" alt="fig14-05.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-05.jpg">Figure 14-5</a> An Inverse Projection Matrix</p>       </div>        <p>In the simplest case,</p>        <p>where <em>a</em> is small enough to fit the entire unit cube. Then we build this inverse projection as the usual projection matrix, as shown here, where matrices are written in a row-major style:</p>        <p>So the formula for the resulting transformed <em>z</em> coordinates, which go into a shadow map, is:</p>        <p><em>Z</em> <em><sub>psm</sub></em> (-<em>a</em>) = 0 <em>,</em> and if we keep decreasing the <em>z</em> value to minus infinity, <em>Z</em> <em><sub>psm</sub></em> tends to ½<em>.</em> The same <em>Z</em> <em><sub>psm</sub></em> =       ½ corresponds to plus infinity, and moving from plus infinity to the far plane increases <em>Z</em> <em><sub>psm</sub></em> to 1 at the far plane. This is why the ray hits all points in the correct order and why there's no need to use       "virtual slides" for creating post-projective space.</p>        <p>This trick works only in post-projective space because normally all points behind the infinity plane have <em>w</em> &lt; 0, so they cannot be rasterized. But for another projection transformation caused by a light camera, these points are       located behind the camera, so the <em>w</em> coordinate is inverted again and becomes positive.</p>        <p>By using this inverse projection matrix, we don't have to use virtual cameras. As a result, we get much better shadow quality without any CPU scene analysis and the associated artifacts.</p>        <p>The only drawback to the inverse projection matrix is that we need a better shadow map depth-value precision, because we use big <em>z</em>-value ranges. However, 24-bit fixed-point depth values are enough for reasonable cases.</p>        <p>Virtual cameras still could be useful, though, because the shadow quality depends on the location of the camera's near plane. The formula for post-projective <em>z</em> is:</p>        <p>As we can see, <em>Q</em> is very close to 1 and doesn't change significantly as long as <em>Z</em> <em><sub>n</sub></em> is much smaller than <em>Z</em> <em><sub>f</sub></em> , which is typical. That's why the near and far planes have to       be changed significantly to affect the <em>Q</em> value, which usually is not possible. At the same time, near-plane values highly influence the post-projective space. For example, for <em>Z</em> <em><sub>n</sub></em> = 1 meter (m), the first       meter in the world space after the near plane occupies half the unit cube in post-projective space. In this respect, if we change <em>Z</em> <em><sub>n</sub></em> to 2 m, we will effectively double the <em>z</em>-value resolution and increase       the shadow quality. That means that we should maximize the <em>Z</em> <em><sub>n</sub></em> value by any means.</p>        <p>The perfect method, proposed in the original PSM article, is to read back the depth buffer, scan through each pixel, and find the maximum possible <em>Z</em> <em><sub>n</sub></em> for each frame. Unfortunately, this method is quite       expensive: it requires reading back a large amount of video memory, causes an additional CPU/GPU stall, and doesn't work well with swizzled and hierarchical depth buffers. So we should use another (perhaps less accurate) method to find a       suitable near-plane value for PSM rendering.</p>        <p>Such other methods for finding a suitable near-plane value for PSM rendering could include various methods of CPU scene analysis:</p>        <ul>         <li>A method based on rough bounding-volume computations (briefly described later in "The Light Camera").</li>          <li>A collision-detection system to estimate the distance to the closest object.</li>          <li>Additional software scene rendering with low-polygon-count level-of-details, which could also be useful for occlusion culling.</li>          <li>Sophisticated analysis based on particular features of scene structure for the specific application. For example, when dealing with scenarios using a fixed camera path, you could precompute the near-plane values for every frame.</li>       </ul>        <p>These methods try to increase the actual near-plane value, but we could also increase the value "virtually." The idea is the same as with the old virtual cameras, but with one difference. When sliding the camera back, we <em>increase</em>       the near-plane value so that the near-plane quads of the original and virtual cameras remain on the same plane. See Figure 14-6.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-06.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-06.jpg" alt="fig14-06.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-06.jpg">Figure 14-6</a> Difference Between Virtual Cameras</p>       </div>        <p>When we slide the virtual camera back, we improve the <em>z</em>-values resolution. However, this makes the value distribution for <em>x</em> and <em>y</em> values worse for near objects, thus balancing shadow quality near and far from the       camera. Because of the very irregular <em>z</em>-value distribution in post-projective space and the large influence of the near-plane value, this balance could not be achieved without this "virtual" slideback. The usual problem of shadows       looking great near the camera but having poor quality on distant objects is the typical result of unbalanced shadow map texel area distribution.</p>        <h4>14.2.2 The Light Camera</h4>        <p>Another problem with PSMs is that the shadow quality relies on the relationship between the light and camera positions. With a vertical directional light, aliasing problems are completely removed, but when light is directed toward the       camera and is close to head-on, there is significant shadow map aliasing.</p>        <p>We're trying to hold the entire unit cube in a single shadow map texture, so we have to make the light's field of view as large as necessary to fit the entire cube. This in turn means that the objects close to the near plane won't receive       enough texture samples. See Figure 14-7.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-07.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-07.jpg" alt="fig14-07.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-07.jpg">Figure 14-7</a> The Light Camera with a Low Light Angle</p>       </div>        <p>The closer the light source is to the unit cube, the poorer the quality. As we know,</p>        <p>so for large outdoor scenes that have <em>Z</em> <em><sub>n</sub></em> = 1 and <em>Z</em> <em><sub>f</sub></em> = 4000, <em>Q</em> = 1.0002, which means that the light source is extremely close to the unit cube. The <em>Z<sub>f</sub></em>       /<em>Z<sub>n</sub></em> correlation is usually bigger than 50, which corresponds to <em>Q</em> = 1.02, which is close enough to create problems.</p>        <p>We'll always have problems fitting the entire unit cube into a single shadow map texture. Two solutions each tackle one part of the problem: <em>Unit cube clipping</em> targets the light camera only on the necessary part of the unit cube,       and the <em>cube map</em> approach uses multiple textures to store depth information.</p>        <h4>Unit Cube Clipping</h4>        <p>This optimization relies on the fact that we need shadow map information only on actual objects, and the volume occupied by these objects is usually much smaller than the whole view frustum volume (especially close to the far plane). That's       why if we tune the light camera to hold real objects only (not the entire unit cube), we'll receive better quality. Of course, we should tune the camera using a simplified scene structure, such as bounding volumes.</p>        <p>Cube clipping was mentioned in the original PSM article, but it took into account all objects in a scene, including shadow casters in the view frustum and potential shadow casters outside the frustum for constructing the virtual camera.       Because we don't need virtual cameras anymore, we can focus the light camera on <em>shadow receivers only</em>, which is more efficient. See Figure 14-8. Still, we should choose near and far clip-plane values for the light camera in       post-projective space to hold all shadow casters in the shadow map. But it doesn't influence shadow quality because it doesn't change the texel area distribution.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-08.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-08.jpg" alt="fig14-08.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-08.jpg">Figure 14-8</a> Focusing the Light Camera Based on the Bounding Volumes of Shadow Receivers</p>       </div>        <p>Because faraway parts of these bounding volumes contract greatly in post-projective space, the light camera's field of view doesn't become very large, even with light sources that are close to the rest of the scene.</p>        <p>In practice, we can use rough bounding volumes to retain sufficient quality—we just need to indicate generally which part of the scene we are interested in. In outdoor scenes, it's the approximate height of objects on the landscape;       in indoor scenes, it's a bounding volume of the current room, and so on.</p>        <p>We'd like to formalize the algorithm of computing the light camera focused on shadow receivers in the scene after we build a set of bounding volumes roughly describing the scene. In fact, the light camera is given by position, direction, up       vector, and projection parameters, most of which are predefined:</p>        <ul>         <li>We can't change the position: it's a light position in post-projective space and nothing else.</li>          <li>In practice, the up vector doesn't change quality significantly, so we can choose anything reasonable.</li>          <li>Projection parameters are entirely defined by the view matrix.</li>       </ul>        <p>So the most interesting thing is choosing the light camera direction based on bounding volumes. The proposed algorithm is this:</p>        <ol>         <li>Compute the vertex list of constructive solid geometry operation, where <em>B</em> <em><sub>i</sub></em> is the <em>i</em>th bounding volume, <em>F</em> is the frustum for every shadow caster bounding volume that we see in the current         frame, and all these operations are performed in a view camera space. Then transform all these vertices into post-projective space. After this step, we have all the points that the light camera should "see." (By the way, we should find a         good near-plane value based on these points, because reading back the depth buffer isn't a good solution.)</li>          <li>Find a light camera. As we already know, this means finding the best light camera direction, because all other parameters are easily computed for a given direction. We propose approximating the optimal direction by the axis of the         minimal cone, centered in the light source and including all the points in the list. The algorithm that finds the optimal cone for a set of points works in linear time, and it is similar to an algorithm that finds the smallest bounding         sphere for a set of points in linear time (Gartner 1999).</li>       </ol>        <p>In this way, we could find an optimal light camera in linear time depending on the bounding volume number, which isn't very large because we need only rough information about the scene structure.</p>        <p>This algorithm is efficient for direct lights in large outdoor scenes. The shadow quality is almost independent of the light angle and slightly decreases if light is directed toward the camera. Figure 14-9 shows the difference between using       unit cube clipping and not using it.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-09a.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-09a.jpg" alt="fig14-09a.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-09a.jpg">Figure 14-9</a> Unit Cube Clipping</p>       </div>        <h4>Using Cube Maps</h4>        <p>Though cube clipping is efficient in some cases, other times it's difficult to use. For example, we might have a densely filled unit cube (which is common), or we may not want to use bounding volumes at all. Plus, cube clipping does not       work with point lights.</p>        <p>A more general method is to use a cube map texture for shadow mapping. Most light sources become point lights in post-projective space, and it's natural to use cube maps for shadow mapping with point light sources. But in post-projective       space, things change slightly and we should use cube maps differently because we need to store information about the unit cube only.</p>        <p>The proposed solution is to use unit cube faces that are back facing, with respect to the light, as platforms for cube-map-face textures.</p>        <p>For a direct light source in post-projective space, the cube map looks like Figure 14-10.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-10.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-10.jpg" alt="fig14-10.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-10.jpg">Figure 14-10</a> Using a Cube Map for Direct Lights</p>       </div>        <p>The number of used cube map faces (ranging from three to five) depends on the position of the light. We use the maximum number of faces when the light is close to the rest of the scene and directed toward the camera, so additional texture       resources are necessary. For other types of light sources located outside the unit cube, the pictures will be similar.</p>        <p>For a point light located inside the unit cube, we should use all six cube map faces, but they're still focused on unit cube faces. See Figure 14-11.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-11.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-11.jpg" alt="fig14-11.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-11.jpg">Figure 14-11</a> Using Cube Maps with a Point Light</p>       </div>        <p>We could say we form a "cube map with displaced center," which is similar to a normal cube map, but with a constant vector added to its texture coordinates. In other words, texture coordinates for cube maps are vertex positions in       post-projective space shifted by the light source position:</p>        <blockquote>         <p><em>Texture coordinates</em> = <em>vertex position - light position</em></p>       </blockquote>        <p>By choosing unit cube faces as the cube map platform, we distribute the texture area proportionally to the screen size and ensure that shadow quality doesn't depend on the light and camera positions. In fact, texel size in post-projective       space is in a guaranteed range, so its projection on the screen depends only on the plane it's projected onto. This projection doesn't stretch texels much, so the texel size on the screen is within guaranteed bounds also.</p>        <p>Because the vertex and pixel shaders are relatively short when rendering the shadow map, what matters is the pure fill rate for the back buffer and the depth shadow map buffer. So there's almost no difference between drawing a single shadow       map and drawing a cube map with the same total texture size (with good occlusion culling, though). The cube map approach has better quality with the same total texture size as a single texture. The difference is the cost of the render target       switch and the additional instructions to compute cube map texture coordinates in the vertex and pixel shaders.</p>        <p>Let's see how to compute these texture coordinates. First, consider the picture shown in Figure 14-12. The blue square is our unit cube, <em>P</em> is the light source point, and <em>V</em> is the point for which we're generating texture       coordinates. We render all six cube map faces in separate passes for the shadow map; the near plane for each pass is shown in green. They're forming another small cube, so <em>Z</em> <sub>1</sub> = <em>Z</em> <em><sub>n</sub></em> /<em>Z</em>       <em><sub>f</sub></em> is constant for every pass.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-12.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-12.jpg" alt="fig14-12.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-12.jpg">Figure 14-12</a> A Detailed Cube Map View in Post-Projective Space</p>       </div>        <p>Now we should compute texture coordinates and depth values to compare for the point <em>V</em>. This just means that we should move this point in the (<em>V</em> - <em>P</em> ) direction until we intersect the cube. Consider <em>d</em>       <sub>1</sub>, <em>d</em> <sub>2</sub>, <em>d</em> <sub>3</sub>, <em>d</em> <sub>4</sub>, <em>d</em> <sub>5</sub>, and <em>d</em> <sub>6</sub> (see the face numbers in Figure 14-12) as the distances from <em>P</em> to each cube map face.</p>        <p>The point on the cube we are looking for (which is also the cube map texture coordinate) is:</p>        <p>Compare the value in the texture against the result of the projective transform of the <em>a</em> value. Because we already divided it by the corresponding <em>d</em> value, thus effectively making <em>Z</em> <em><sub>f</sub></em> = 1 and       <em>Z</em> <em><sub>n</sub></em> = <em>Z</em> <sub>1</sub>, all we have to do is apply that projective transform. Note that in the case of the inverse camera projection from Section 14.2.1, <em>Z</em> <em><sub>n</sub></em> = -<em>Z</em>       <sub>1</sub>, <em>Z</em> <em><sub>f</sub></em> = <em>Z</em> <sub>1</sub>.</p>        <p>(All these calculations are made in OpenGL-like coordinates, where the unit cube is actually a unit cube. In Direct3D, the unit cube is half the size, because the <em>z</em> coordinate is in the [0..1] range.)</p>        <p>Listing 14-1 is an example of how the shader code might look.</p>        <h4>Example 14-1. Shader Code for Computing Cube Map Texture Coordinates</h4>       <pre> <strong><em>// c[d1] = 1/d1, 1/d2, 1/d3, 0</em></strong> <strong><em>// c[d2] = -1/d4, -1/d5, -1/d6, 0</em></strong> <strong><em>// c[z] = Q, -Q * Zn, 0, 0</em></strong> <strong><em>// c[P] = P</em></strong> <strong><em>// r[V] = V</em></strong> <strong><em>// cbmcoord - output cube map texture coordinates</em></strong> <strong><em>// depth - depth to compare with shadow map values</em></strong>  <strong><em>//Per-vertex level</em></strong> <strong><em>sub</em></strong> r[VP], r[V], c[P] <strong><em>mul</em></strong> r1, r[VP], c[d1] <strong><em>mul</em></strong> r2, r[VP], c[d2] <strong><em>//Per-pixel level</em></strong> <strong><em>max</em></strong> r3, r1, r2 <strong><em>max</em></strong> r3.x, r3.x, r3.y <strong><em>max</em></strong> r3.x, r3.x, r3.z <strong><em>rcp</em></strong> r3.w, r3.x <strong><em>mad</em></strong> cbmcoord, r[VP], r3.w, c[P] <strong><em>rcp</em></strong> r3.x, r3.w <strong><em>mad</em></strong> depth, r3.x, c[z].x, c[z].y </pre>        <p>Because depth textures cannot be cube maps, we could use color textures, packing depth values into the color channels. There are many ways to do this and many implementation-dependent tricks, but their description is out of the scope of       this chapter.</p>        <p>Another possibility is to emulate this cube map approach with multitexturing, in which every cube map face becomes an independent texture (depth textures are great in this case). We form several texture coordinate sets in the vertex shader       and multiply by the shadow results in the pixel shader. The tricky part is to manage these textures over the objects in the scene, because every object rarely needs all six faces.</p>        <h4>14.2.3 Biasing</h4>        <p>As we stated earlier, the constant bias that is typically used in uniform shadow maps cannot be used with PSMs because the <em>z</em> values and the texel area distributions vary greatly with different light positions and points in the       scene.</p>        <p>If you plan to use depth textures, try <em>z</em> slope–scaled bias for biasing. It's often enough to fix the artifacts, especially when very distant objects don't fall within the camera. However, some cards do not support depth       textures (in DirectX, depth textures are supported only by NVIDIA cards), and depth textures can't be a cube map. In these cases, you need a different, more general algorithm for calculating bias. Another difficulty is that it's hard to       emulate and tweak <em>z</em> slope–scaled bias because it requires additional data—such as the vertex coordinates of the current triangle—passed into the pixel shader, plus some calculations, which isn't robust at all.</p>        <p>Anyway, let's see why we can't use constant bias anymore. Consider these two cases: the light source is near the unit cube, and the light source is far from the unit cube. See Figure 14-13.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-13.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-13.jpg" alt="fig14-13.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-13.jpg">Figure 14-13</a> Light Close to and Far from the Unit Cube</p>       </div>        <p>The problem is that the <em>Z</em> <em><sub>f</sub></em> /<em>Z</em> <em><sub>n</sub></em> correlation, which determines the <em>z</em>-value distribution into a shadow map, varies a lot in these two cases. So the constant bias would mean a       totally different actual bias in world and post-projective space: The constant bias tuned to the first light position won't be correct for the second light, and vice versa. Meanwhile, <em>Z</em> <em><sub>f</sub></em> /<em>Z</em>       <em><sub>n</sub></em> changes a lot, because the light source could be close to the unit cube and could be distant (even at infinity), depending on the relative positions of the light and the camera in world space.</p>        <p>Even with a fixed light source position, sometimes we cannot find a suitable constant for the bias. The bias should depend on the point position—because the projective transform enlarges the near objects and shrinks the far       ones—so the bias should be smaller near the camera and bigger for distant objects. Figure 14-14 shows the typical artifacts of using a constant bias in this situation.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-14.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-14.jpg" alt="fig14-14.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-14.jpg">Figure 14-14</a> Artifacts with Constant Bias</p>       </div>        <p>In short, the proposed solution is to use biasing in world space (and not to analyze the results of the double-projection matrix) and then transform this world-space bias in post-projective space. The computed value depends on the double       projection, and it's correct for any light and camera position. These operations could be done easily in a vertex shader. Furthermore, this world-space bias value should be scaled by texel size in world space to deal with artifacts caused by       the distribution of nonuniform texel areas.</p>        <blockquote>         <p><em>P<sub>biased</sub></em> = (P <em><sub>orig</sub></em> + <em>L</em>(<em>a</em> + <em>bL<sub>texel</sub></em> ))<em>M</em>,</p>       </blockquote>        <p>where <em>P</em> <em><sub>orig</sub></em> is the original point position, <em>L</em> is the light vector direction in world space, <em>L<sub>texel</sub></em> is the texel size in world space, <em>M</em> is the final shadow map matrix, and       <em>a</em> and <em>b</em> are bias coefficients.</p>        <p>The texel size in world space could be approximately computed with simple matrix calculations. First, transform the point into shadow map space, and then shift this point by the texel size without changing depth. Next, transform it back       into world space and square the length of the difference between this point and the original one. This gives us <em>L<sub>texel</sub></em> :</p>        <p>and <em>S</em> <em><sub>x</sub></em> and <em>S</em> <em><sub>y</sub></em> are shadow map resolutions.</p>        <p>Obviously, we can build a single matrix that performs all the transformations (except multiplying the coordinates, of course):</p>        <p>where <em>M'</em> includes transforming, shifting, transforming back, and subtracting.</p>        <p>This turns out to be a rather empirical solution, but it should still be tweaked for your particular needs. See Figure 14-15.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-15.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-15.jpg" alt="fig14-15.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-15.jpg">Figure 14-15</a> Bias Calculated in the Vertex Shader</p>       </div>        <p>The vertex shader code that performs these calculations might look like Listing 14-2.</p>        <h4>Example 14-2. Calculating Bias in a Vertex Shader</h4>       <pre> <strong><em>def</em></strong> c0, a, b, 0 ,0 <strong><em>// Calculating Ltexel</em></strong> <strong><em>dp4</em></strong> r1.x, v0, c[LtexelMatrix_0] <strong><em>dp4</em></strong> r1.y, v0, c[LtexelMatrix_1] <strong><em>dp4</em></strong> r1.z, v0, c[LtexelMatrix_2] <strong><em>dp4</em></strong> r1.w, v0, c[LtexelMatrix_3] <strong><em>// Transforming homogeneous coordinates</em></strong> <strong><em>// (in fact, we often can skip this step)</em></strong> <strong><em>rcp</em></strong> r1.w, r1.w <strong><em>mul</em></strong> r1.xy, r1.w, r1.xy <strong><em>// Now r1.x is an Ltexel</em></strong> <strong><em>mad</em></strong> r1.x, r1.x, c0.x, c0.y <strong><em>dp3</em></strong> r1.x, r1, r1 <strong><em>// Move vertex in world space</em></strong> <strong><em>mad</em></strong> r1, v0, c[Lightdir], r1.x <strong><em>// Transform vertex into post-projective space</em></strong> <strong><em>// (we need z and w only)</em></strong> <strong><em>dp4</em></strong> r[out].z, r1, c[M_2] <strong><em>dp4</em></strong> r[out].w, r1, c[M_3] </pre>        <p>The <tt>r[out]</tt> register holds the result of the biasing: the depth value, and the corresponding <em>w</em>, that should be interpolated across the triangle. Note that this interpolation should be separate from the interpolation of       texture coordinates (<em>x, y</em>, and the corresponding <em>w</em>), because these <em>w</em> coordinates are different. This biased value could be used when comparing with the shadow map value, or during the actual shadow map rendering (the       shadow map holds biased values).</p>        <h2>14.3 Tricks for Better Shadow Maps</h2>        <p>The advantage of shadow mapping over shadow volumes is the potential to create a color gradient between "shadowed" and "nonshadowed" samples, thus simulating soft shadows. This shadow "softness" doesn't depend on distance from the occluder,       light source size, and so on, but it still works in world space. Blurring stencil shadows, on the other hand, is more difficult, although Assarsson et al. (2003) make significant progress.</p>        <p>This section covers methods of filtering and blurring shadow maps to create a fake shadow softness that has a constant range of blurring but still looks good.</p>        <h4>14.3.1 Filtering</h4>        <p>Most methods of shadow map filtering are based on the <em>percentage-closer filtering</em> (PCF) principle. The only difference among the methods is how the hardware lets us use it. NVIDIA depth textures perform PCF after comparison with       the depth value; on other hardware, we have to take several samples from the nearest texels and average their results (for true PCF). In general, the depth texture filtering is more efficient than the manual PCF technique with four samples.       (PCF needs about eight samples to produce comparable quality.) In addition, using depth texture filtering doesn't forbid PCF, so we can take several filtered samples to further increase shadow quality.</p>        <p>Using PCF with PSMs is no different from using it with standard shadow maps: samples from neighboring texels are used for filtering. On the GPU, this is achieved by shifting texture coordinates one texel in each direction. For a more       detailed discussion of PCF, see Chapter 11, "Shadow Map Antialiasing."</p>        <p>The shader pseudocode for PCF with four samples looks like Listings 14-3 and 14-4.</p>        <p>These tricks improve shadow quality, but they do not hide serious aliasing problems. For example, if many screen pixels map to one shadow map texel, large stair-stepping artifacts will be visible, even if they are somewhat blurred. Figure       14-16 shows an aliased shadow without any filtering, and Figure 14-17 shows how PCF helps improve shadow quality but cannot completely remove aliasing artifacts.</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-16.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-16.jpg" alt="fig14-16.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-16.jpg">Figure 14-16</a> Strong Aliasing</p>       </div>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-17.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-17.jpg" alt="fig14-17.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-17.jpg">Figure 14-17</a> Filtered Aliasing</p>       </div>        <h4>Example 14-3. Vertex Shader Pseudocode for PCF</h4>       <pre> <strong><em>def</em></strong> c0, sample1x, sample1Y, 0, 0 <strong><em>def</em></strong> c1, sample2x, sample2Y, 0, 0 <strong><em>def</em></strong> c2, sample3x, sample3Y, 0, 0 <strong><em>def</em></strong> c3, sample4x, sample4Y, 0, 0 <strong><em>// The simplest case:</em></strong> <strong><em>// def c0, 1 / shadowmapsizeX, 1 / shadowmapsizeY, 0, 0</em></strong> <strong><em>// def c1, -1 / shadowmapsizeX, -1 / shadowmapsizeY, 0, 0</em></strong> <strong><em>// def c2, -1 / shadowmapsizeX, 1/ shadowmapsizeY, 0, 0</em></strong> <strong><em>// def c3, 1 / shadowmapsizeX, -1 / shadowmapsizeY, 0, 0</em></strong> . . . <strong><em>// Point - vertex position in light space</em></strong> <strong><em>mad</em></strong> oT0, point.w, c0, point <strong><em>mad</em></strong> oT1, point.w, c1, point <strong><em>mad</em></strong> oT2, point.w, c2, point <strong><em>mad</em></strong> oT3, point.w, c3, point </pre>        <h4>Example 14-4. Pixel Shader Pseudocode for PCF</h4>       <pre> <strong><em>def</em></strong> c0, 0.25, 0.25, 0.25, 0.25 <strong><em>tex</em></strong> t0 <strong><em>tex</em></strong> t1 <strong><em>tex</em></strong> t2 <strong><em>tex</em></strong> t3 . . . <strong><em>// After depth comparison</em></strong> <strong><em>mul</em></strong> r0, t0, c0 <strong><em>mad</em></strong> r0, t1, c0, r0 <strong><em>mad</em></strong> r0, t2, c0, r0 <strong><em>mad</em></strong> r0, t3, c0, r0 </pre>        <h4>14.3.2 Blurring</h4>        <p>As we know from projective shadows, the best blurring results often come from rendering to a smaller resolution texture with a pixel shader blur, then feeding this resulting texture back through the blur pixel shader several times (known as       <em>ping-pong rendering</em>). Shadow mapping and projective shadows are similar techniques, so why can't we use this method? The answer: because the shadow map isn't a black-and-white picture; it's a collection of depth values, and "blurring       a depth map" doesn't make sense.</p>        <p>In fact, the proposal is to use the color part of the shadow map render (which comes almost for free) as projective texture for some objects. For example, assume that we have an outdoor landscape scene and we want a high-quality blurred       shadow on the ground because ground shadows are the most noticeable.</p>        <ol type="1">         <li>Before rendering the depth shadow map, clear the color buffer with 1. During the render, draw 0 into the color buffer for every object except the landscape; for the landscape, draw 1 in color. After the whole shadow map renders, we'll         have 1 where the landscape is nonshadowed and 0 where it's shadowed. See Figure 14-18.            <div class="figure">             <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-18.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-18.jpg" alt="fig14-18.jpg"></a>              <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-18.jpg">Figure 14-18</a> The Original Color Part for a Small Test Scene</p>           </div>         </li>          <li>Blur the picture (the one in Figure 14-18) severely, using multiple passes with a simple blur pixel shader. For example, using a simple two-pass Gaussian blur gives good results. (You might want to adjust the blurring radius for distant         objects.) After this step, we'll have a high-quality blurred texture, as shown in Figure 14-19.            <div class="figure">             <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-19.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-19.jpg" alt="fig14-19.jpg"></a>              <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-19.jpg">Figure 14-19</a> The Blurred Color Part for a Small Test Scene</p>           </div>         </li>          <li>While rendering the scene with shadows, render the landscape with the blurred texture instead of the shadow map, and render all other objects with the depth part of the shadow map. See Figure 14-20.            <div class="figure">             <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-20.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-20.jpg" alt="fig14-20.jpg"></a>              <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-20.jpg">Figure 14-20</a> Applying Blurring to a Real Scene</p>           </div>         </li>       </ol>        <p>The difference in quality is dramatic.</p>        <p>Of course, we can use this method not only with landscapes, but also with any object that does not need self-shadowing (such as floors, walls, ground planes, and so on). Fortunately, in these areas shadows are most noticeable and aliasing       problems are most evident. Because we have several color channels, we can blur shadows on several objects at the same time:</p>        <ul>         <li>Using depth textures, the color buffer is completely free, so we can use all four channels for four objects.</li>          <li>For floating-point textures, one channel stores depth information, so we have three channels for blurring.</li>          <li>For fixed-point textures, depth is usually stored in the red and green channels, so we have only two free channels.</li>       </ul>        <p>This way we'll have nice blurred shadows on the ground, floor, walls, and so on while retaining all other shadows (blurred with PCF) on other objects (with proper self-shadowing).</p>        <h2>14.4 Results</h2>        <p>The screenshots in Figures 14-21, 14-22, 14-23, and 14-24 were captured on the NVIDIA GeForce4 Ti 4600 in 1600x1200 screen resolution, with 100,000 to 500,000 visible polygons. All objects receive and cast shadows with real-time frame rates       (more than 30).</p>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-21.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-21.jpg" alt="fig14-21.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-21.jpg">Figure 14-21</a></p>       </div>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-22.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-22.jpg" alt="fig14-22.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-22.jpg">Figure 14-22</a></p>       </div>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-23.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-23.jpg" alt="fig14-23.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-23.jpg">Figure 14-23</a></p>       </div>        <div class="figure">         <a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-24.jpg"><img src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-24.jpg" alt="fig14-24.jpg"></a>          <p><a href="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/fig14-24.jpg">Figure 14-24</a></p>       </div>        <h2>14.5 References</h2>        <p><a name="ch14bib01_01" id="ch14bib01_01"></a>Assarsson, U., M. Dougherty, M. Mounier, and T. Akenine-Möller. 2003. "An Optimized Soft Shadow Volume Algorithm with Real-Time Performance." In <em>Proceedings of the SIGGRAPH/Eurographics       Workshop on Graphics Hardware 2003</em>.</p>        <p><a name="ch14bib01_02" id="ch14bib01_02"></a>Gartner, Bernd. 1999. "<em>Smallest Enclosing Balls: Fast and Robust in C++</em>." Web page. <strong><a href="http://www.inf.ethz.ch/personal/gaertner/texts/own_work/esa99_final.pdf">http://www.inf.ethz.ch/personal/gaertner/texts/own_work/esa99_final.pdf</a></strong></p>        <p><a name="ch14bib01_03" id="ch14bib01_03"></a>Stamminger, Marc, and George Drettakis. 2002. "Perspective Shadow Maps." In <em>Proceedings of ACM SIGGRAPH 2002</em>.</p>        <p><em>The author would like to thank Peter Popov for his many helpful and productive discussions.</em></p> <!-- generated html end -->       <!-- Copyright info for The Cg Tutorial -->       <hr>        <h4>Copyright</h4>        <p>Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and Addison-Wesley was aware of a trademark claim, the designations have been       printed with initial capital letters or in all capitals.</p>        <p>The authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential       damages in connection with or arising out of the use of the information or programs contained herein.</p>        <p>The publisher offers discounts on this book when ordered in quantity for bulk purchases and special sales. For more information, please contact:</p>        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;U.S.&nbsp;Corporate&nbsp;and&nbsp;Government&nbsp;Sales<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(800)&nbsp;382-3419<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:corpsales@pearsontechgroup.com">corpsales@pearsontechgroup.com</a></p>        <p>For sales outside of the U.S., please contact:</p>        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;International&nbsp;Sales<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:international@pearsoned.com">international@pearsoned.com</a></p>        <p>Visit Addison-Wesley on the Web: <a href="http://www.awprofessional.com/">www.awprofessional.com</a></p>        <p>Library of Congress Control Number: 2004100582</p>        <p>GeForce™&nbsp;and&nbsp;NVIDIA&nbsp;Quadro<sup>®</sup>&nbsp;are&nbsp;trademarks&nbsp;or&nbsp;registered&nbsp;trademarks&nbsp;of&nbsp;NVIDIA&nbsp;Corporation.<br>       RenderMan<sup>®</sup>&nbsp;is&nbsp;a&nbsp;registered&nbsp;trademark&nbsp;of&nbsp;Pixar&nbsp;Animation&nbsp;Studios.<br>       "Shadow&nbsp;Map&nbsp;Antialiasing"&nbsp;©&nbsp;2003&nbsp;NVIDIA&nbsp;Corporation&nbsp;and&nbsp;Pixar&nbsp;Animation&nbsp;Studios.<br>       "Cinematic&nbsp;Lighting"&nbsp;©&nbsp;2003&nbsp;Pixar&nbsp;Animation&nbsp;Studios.<br>       Dawn&nbsp;images&nbsp;©&nbsp;2002&nbsp;NVIDIA&nbsp;Corporation.&nbsp;Vulcan&nbsp;images&nbsp;©&nbsp;2003&nbsp;NVIDIA&nbsp;Corporation.</p>        <p>Copyright © 2004 by NVIDIA Corporation.</p>        <p>All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior consent of the       publisher. Printed in the United States of America. Published simultaneously in Canada.</p>        <p>For information on obtaining permission for use of material from this work, please submit a written request to:</p>        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pearson&nbsp;Education,&nbsp;Inc.<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rights&nbsp;and&nbsp;Contracts&nbsp;Department<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One&nbsp;Lake&nbsp;Street<br>       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Upper&nbsp;Saddle&nbsp;River,&nbsp;NJ&nbsp;07458</p>        <p>Text printed on recycled and acid-free paper.</p>        <p>5 6 7 8 9 10 QWT 09 08 07</p>        <p>5th Printing September 2007</p> <!-- <div align="right" style=" color:#999999;">Last Update: 15:05 05/12/2008</div> -->    </div>
    <div class="col-md-4">
      <ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/contributors" class="">Contributors</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/copyright" class="">Copyright</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/foreword" class="">Foreword</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects" class="">Part I: Natural Effects</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-1-effective-water-simulation-physical-models" class="">Chapter 1. Effective Water Simulation from Physical Models</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-2-rendering-water-caustics" class="">Chapter 2. Rendering Water Caustics</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-3-skin-dawn-demo" class="">Chapter 3. Skin in the "Dawn" Demo</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-4-animation-dawn-demo" class="">Chapter 4. Animation in the "Dawn" Demo</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-5-implementing-improved-perlin-noise" class="">Chapter 5. Implementing Improved Perlin Noise</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-6-fire-vulcan-demo" class="">Chapter 6. Fire in the "Vulcan" Demo</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-7-rendering-countless-blades-waving-grass" class="">Chapter 7. Rendering Countless Blades of Waving Grass</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-i-natural-effects/chapter-8-simulating-diffraction" class="">Chapter 8. Simulating Diffraction</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows" class="active">Part II: Lighting and Shadows</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-10-cinematic-lighting" class="">Chapter 10. Cinematic Lighting</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-11-shadow-map-antialiasing" class="">Chapter 11. Shadow Map Antialiasing</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-12-omnidirectional-shadow-mapping" class="">Chapter 12. Omnidirectional Shadow Mapping</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-13-generating-soft-shadows-using-occlusion" class="">Chapter 13. Generating Soft Shadows Using Occlusion Interval Maps</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding" class="active">Chapter 14. Perspective Shadow Maps: Care and Feeding</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-15-managing-visibility-pixel-lighting" class="">Chapter 15. Managing Visibility for Per-Pixel Lighting</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-9-efficient-shadow-volume-rendering" class="">Chapter 9. Efficient Shadow Volume Rendering</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials" class="">Part III: Materials</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials/chapter-16-real-time-approximations-subsurface-scattering" class="">Chapter 16. Real-Time Approximations to Subsurface Scattering</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials/chapter-17-ambient-occlusion" class="">Chapter 17. Ambient Occlusion</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials/chapter-18-spatial-brdfs" class="">Chapter 18. Spatial BRDFs</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials/chapter-19-image-based-lighting" class="">Chapter 19. Image-Based Lighting</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iii-materials/chapter-20-texture-bombing" class="">Chapter 20. Texture Bombing</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing" class="">Part IV: Image Processing</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-21-real-time-glow" class="">Chapter 21. Real-Time Glow</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-22-color-controls" class="">Chapter 22. Color Controls</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-23-depth-field-survey-techniques" class="">Chapter 23. Depth of Field: A Survey of Techniques</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-24-high-quality-filtering" class="">Chapter 24. High-Quality Filtering</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-25-fast-filter-width-estimates-texture-maps" class="">Chapter 25. Fast Filter-Width Estimates with Texture Maps</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-26-openexr-image-file-format" class="">Chapter 26. The OpenEXR Image File Format</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-27-framework-image-processing" class="">Chapter 27. A Framework for Image Processing</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities" class="">Part V: Performance and Practicalities</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-28-graphics-pipeline-performance" class="">Chapter 28. Graphics Pipeline Performance</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-29-efficient-occlusion-culling" class="">Chapter 29. Efficient Occlusion Culling</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-30-design-fx-composer" class="">Chapter 30. The Design of FX Composer</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-31-using-fx-composer" class="">Chapter 31. Using FX Composer</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-32-introduction-shader-interfaces" class="">Chapter 32. An Introduction to Shader Interfaces</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-33-converting-production-renderman" class="">Chapter 33. Converting Production RenderMan Shaders to Real-Time</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-34-integrating-hardware-shading-cinema" class="">Chapter 34. Integrating Hardware Shading into Cinema 4D</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-35-leveraging-high-quality-software" class="">Chapter 35. Leveraging High-Quality Software Rendering Effects in Real-Time Applications</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-v-performance-and-practicalities/chapter-36-integrating-shaders-applications" class="">Chapter 36. Integrating Shaders into Applications</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles" class="">Part VI: Beyond Triangles</a><ul><li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/appendix" class="">Appendix</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-37-toolkit-computation-gpus" class="">Chapter 37. A Toolkit for Computation on GPUs</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-38-fast-fluid-dynamics-simulation-gpu" class="">Chapter 38. Fast Fluid Dynamics Simulation on the GPU</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-39-volume-rendering-techniques" class="">Chapter 39. Volume Rendering Techniques</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-40-applying-real-time-shading-3d-ultrasound" class="">Chapter 40. Applying Real-Time Shading to 3D Ultrasound Visualization</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-41-real-time-stereograms" class="">Chapter 41. Real-Time Stereograms</a></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/part-vi-beyond-triangles/chapter-42-deformers" class="">Chapter 42. Deformers</a></li>
</ul></li>
<li><a href="https://developer.nvidia.com/gpugems/gpugems/preface" class="">Preface</a></li>
</ul>    </div>
  </div>
</div>
</div>
</section>
  </div>
                  </section>

        
      </div>
    </div>
    <div class="separator"></div>
  </div>
  
      <footer>
      <div class="footer-links">
        <div class="container">
          <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-3 col-lg-3">
              <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="padding-md-footer">
                  <div class="logo-footer"></div>
                </div>
              </div>
              <div class="col-xs-12 col-sm-12 col-md-9 col-lg-9 padding-section-footer">
                  <div class="region region-footer-menu">
    <div class="block block-menu" id="block-menu-menu-footer-menu">
  <div class="block-content zone-select">
    <ul class="menu nav"><li class="first leaf"><a href="https://developer.nvidia.com/hpc" title="">HIGH PERFORMANCE COMPUTING</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworks" title="">GAMEWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/embedded-computing" title="">JETPACK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/designworks" title="">DESIGNWORKS</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/drive" title="">DRIVE</a></li>
</ul>  </div>
</div>
  </div>
              </div>
            </div>
            <div class="col-xs-12 col-sm-12 col-md-9 col-lg-9">
                                                        </div>
          </div>
        </div>
      </div>

      <div class="footer-boilerplate">
        <div class="container">
          <div class="boilerplate">
            <div class="col-xs-12 col-sm-12 col-lg-9 padding-sm-bottom">
              Copyright © 2020 NVIDIA Corporation                              <ul class="legal_links"><li class="first leaf"><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/" title="">Legal Information</a></li>
<li class="leaf"><a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" title="">Privacy Policy</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/contact" title="">Contact</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" title="NVIDIA websites use cookies to deliver and improve the website experience. See our cookie policy for further details on how we use cookies and how to change your cookie settings.">Cookie policy</a></li>
</ul>                          </div>
          </div>
        </div>
      </div>
    </footer>
  </div>
  <script>var dzauth = {"settings":{"client_id":"4jljTejN7RMO9suL0S33gFrYgjHX0VcW","redirect_uri":"https:\/\/developer.nvidia.com\/auth0\/callback","custom_domain":"login.developer.nvidia.com","domain":"devzone.auth0.com","auto_login":true}};

function nvidia_dzauth_register_and_redirect(redirect_destination) {
  if(redirect_destination) {
    history.pushState(null, '', redirect_destination);
    showDzAuth('login');
  }
}

function nvidia_dzauth_init() {
  initDzAuth(dzauth);
  if (typeof auth0 !== 'undefined') {
    dzCheckSession(auth0);
  }
}
nvidia_dzauth_init();
</script>
<script>_satellite.pageBottom();</script>
<script src="./Chapter 14. Perspective Shadow Maps_ Care and Feeding _ NVIDIA Developer_files/js_HsOY2zjRl9nOeGgDfOD_SOf9C_rAG_RRUdsGS8GLqpE.js.download"></script>
<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"beacon":"bam.nr-data.net","licenseKey":"6f2048d7bc","applicationID":"341156206","transactionName":"YFVbbEJQXhJTW0JRX1kfeFtEWF8PHUxXQF9ZX1RBb1VZEkJUV0FvQ1FBV15eXRhtTFNKXWhAWF9V","queueTime":0,"applicationTime":283,"atts":"TBJYGgpKTRw=","errorBeacon":"bam.nr-data.net","agent":""}</script>

<div id="cboxOverlay" style="display: none;"></div><div id="colorbox" class="" role="dialog" tabindex="-1" style="display: none;"><div id="cboxWrapper"><div><div id="cboxTopLeft" style="float: left;"></div><div id="cboxTopCenter" style="float: left;"></div><div id="cboxTopRight" style="float: left;"></div></div><div style="clear: left;"><div id="cboxMiddleLeft" style="float: left;"></div><div id="cboxContent" style="float: left;"><div id="cboxTitle" style="float: left;"></div><div id="cboxCurrent" style="float: left;"></div><button type="button" id="cboxPrevious"></button><button type="button" id="cboxNext"></button><button id="cboxSlideshow"></button><div id="cboxLoadingOverlay" style="float: left;"></div><div id="cboxLoadingGraphic" style="float: left;"></div></div><div id="cboxMiddleRight" style="float: left;"></div></div><div style="clear: left;"><div id="cboxBottomLeft" style="float: left;"></div><div id="cboxBottomCenter" style="float: left;"></div><div id="cboxBottomRight" style="float: left;"></div></div></div><div style="position: absolute; width: 9999px; visibility: hidden; display: none; max-width: none;"></div></div><div id="sidr" class="sidr left"><div class="sidr-inner">

                  <ul class="menu nav navbar-nav primary"><li class="first expanded megamenu dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Solutions <span class="caret"></span></a><div class="dropdown-menu"><div class="container-fluid"><ul class="navbar-nav"><li class="first expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">AI and Deep Learning</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/deep-learning">Deep Learning</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/machine-learning">Machine Learning</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/tensorrt">Inference</a></li>
<li class="leaf"><a href="https://www.nvidia.com/en-us/deep-learning-ai/education" title="">Deep Learning Institute</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/Clara-Genomics">Genomics</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/gpu-cloud/" title="">GPU-optimized S/W (NGC)</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/embedded-computing">Autonomous Machines</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/embedded/develop/hardware">Hardware (Jetson)</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk" title="">Robotics</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">Video Analytics</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/drive">Autonomous Vehicles</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/drive/drive-agx">Hardware (DRIVE AGX)</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-hyperion" title="">Reference Architecture</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-software">Autonomous Vehicle Software</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">Data Center Simulation Platform</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/rtx">Graphics and Simulation</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/graphics-research-tools" title="">Graphics Research Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/video-processing">Video Processing</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/rtx/raytracing" title="">Ray Tracing</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/rtx/ngx" title="">AI for Graphics</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworks-visualfx-overview" title="">Real-time VFX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks">Virtual and Augmented Reality</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/physx-sdk">Simulation</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara">Medical Imaging</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nvidia-index">Scientific Visualization</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/design-visualization/solutions/quadro-display-desktop-management#Management">Display</a></li>
</ul></li>
<li class="expanded dropdown"><a href="https://developer.nvidia.com/hpc">High-performance Computing</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/language-solutions">Languages and APIs</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gpu-accelerated-libraries" title="">GPU Accelerated Libraries</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/openacc">OpenACC Programming Model</a></li>
</ul></li>
<li class="last expanded dropdown"><a href="https://developer.nvidia.com/tools-overview">Tools and Management</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/tools-overview" title="">Developer Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/what-is-designworks">Management Tools</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/tools-overview">Android and Tegra for Mobile</a></li>
</ul></li>
</ul></div></div></li>
<li class="expanded megamenu dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Platforms <span class="caret"></span></a><div class="dropdown-menu"><div class="container-fluid"><ul class="navbar-nav"><li class="first expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">CUDA-X AI</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/tensorrt">TensorRT</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cudnn">cuDNN</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nccl">NCCL</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cublas">cuBLAS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/cusparse">cuSPARSE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/opticalflow-sdk">Optical Flow SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/DALI">DALI</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/transfer-learning-toolkit">Transfer Learning Toolkit</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/digits">DIGITS</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">CLARA</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/clara-guardian" title="">Clara Guardian</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara-medical-imaging" title="">Clara Imaging</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/clara-parabricks">Clara Parabricks</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">HPC</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/cuda-zone">CUDA Toolkit</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/openacc">OpenACC</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">DRIVE</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/drive/drive-agx">DRIVE AGX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-hyperion">DRIVE Hyperion</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">DRIVE Sim</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/drive-constellation">DRIVE Constellation</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/data-center/dgx-systems/">DGX</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">RTX</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/optix">OptiX SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks/vrworks-audio" title="">Path-traced Audio (VRWorks) </a></li>
<li class="leaf"><a href="https://developer.nvidia.com/Vulkan">VKRay</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/mdl-sdk" title="">MDL SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vmaterials">vMaterials</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/physx-sdk">PhysX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/flex">Flex</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/opticalflow-sdk" title="">Optical Flow SDK</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/nvidia-video-codec-sdk">Video Codec SDK</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/gpudirectforvideo">GPUDirect for Video</a></li>
</ul></li>
<li class="expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">ISAAC</a><ul><li class="first leaf"><a href="https://developer.nvidia.com/embedded/develop/hardware">Jetson Developer Kits</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/embedded/jetpack">Jetpack</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac Robot Engine</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac Sim</a></li>
</ul></li>
<li class="last expanded dropdown"><a class="" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Metropolis</a><ul><li class="first last leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">DeepStream SDK</a></li>
</ul></li>
</ul></div></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Documentation <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://docs.nvidia.com/">Library</a></li>
<li class="leaf"><a href="https://raytracing-docs.nvidia.com/">Ray Tracing</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/cuda/index.html">CUDA Toolkit</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/gameworks/index.html">GameWorks</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/drive/index.html">DRIVE</a></li>
<li class="leaf"><a href="https://docs.nvidia.com/ngc/index.html">NGC</a></li>
<li class="last leaf"><a href="https://docs.nvidia.com/isaac/index.html">Isaac</a></li>
</ul></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Downloads <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://developer.nvidia.com/cuda-toolkit" title="">CUDA Toolkit</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworksdownload#?tx=$gameworks,developer_tools" title="">Developer Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara">CLARA</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive/downloads">DRIVE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/graphics-research-tools" title="">Graphics Research Tools</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworksdownload">Gameworks</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/isaac-sdk">Isaac</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/embedded/downloads">Jetson</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/deepstream-sdk">Metropolis</a></li>
</ul></div></li>
<li class="expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Resources <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://developer.nvidia.com/contact">Contact Us</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/developer-program">Developer Program</a></li>
<li class="leaf"><a href="https://courses.nvidia.com/">Deep Learning Institute</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/higher-education-and-research">Educators</a></li>
<li class="leaf"><a href="https://ngc.nvidia.com/">NGC</a></li>
<li class="leaf"><a href="https://resources.nvidia.com/l/events" title="">Event Recordings</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/open-source">Open Source</a></li>
<li class="last leaf"><a href="https://www.nvidia.com/en-us/deep-learning-ai/startups/" title="">AI Startups</a></li>
</ul></div></li>
<li class="last expanded dropdown"><a class="dropdown-toggle" data-target="#" data-toggle="dropdown" href="https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-14-perspective-shadow-maps-care-and-feeding#">Community <span class="caret"></span></a><div class="dropdown-menu"><ul><li class="first leaf"><a href="https://forums.developer.nvidia.com/" title="">Forums</a></li>
<li class="leaf"><a href="https://devblogs.nvidia.com/">Blog</a></li>
<li class="last leaf"><a href="https://news.developer.nvidia.com/">News</a></li>
</ul></div></li>
</ul>                <ul class="nav navbar-nav navbar-margin navbar-right navbar-margin-media login-nav">
                      <li class="search" id="search-top">
              <div class="search-form" id="search-top-form">
                <form class="gss form-search content-search" action="https://developer.nvidia.com/search" method="post" id="nvidia-site-search-form" accept-charset="UTF-8"><div><div class="input-group"><input placeholder="Search..." class="form-control form-text" type="text" id="edit-term" name="term" value="" size="15" maxlength="128"><span class="input-group-btn"><button type="submit" class="btn btn-default"><span class="icon glyphicon glyphicon-search" aria-hidden="true"></span>
</button></span></div><button class="btn element-invisible btn-primary form-submit" type="submit" id="edit-submit" name="op" value="Search">Search</button>
<input type="hidden" name="form_build_id" value="form-Pwhi73Mi4C3SFuYLq5o5BQB6iwi9QOLBZjdeeOUiabg">
<input type="hidden" name="form_id" value="nvidia_site_search_form">
</div></form>              </div>
            </li>
          
          
          
                      <li class="leaf last" id="dzauth_login_link"><a href="javascript:jQuery.sidr(&#39;close&#39;);javascript:showDzAuth(&#39;login&#39;);">Account</a></li>
          
        </ul>
      </div><div class="sidr-inner">
          <ul class="menu nav navbar-nav secondary"><li class="first leaf"><a href="https://developer.nvidia.com/rtx" title="">RTX</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/gameworks" title="">GAMEWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/designworks" title="">DESIGNWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/vrworks" title="">VRWORKS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/hpc" title="">HPC</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/deepstream-sdk" title="">METROPOLIS</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/drive" title="">DRIVE</a></li>
<li class="leaf"><a href="https://developer.nvidia.com/clara" title="">CLARA</a></li>
<li class="last leaf"><a href="https://developer.nvidia.com/open-source" title="">OPEN SOURCE</a></li>
</ul>                  </div></div><div class="nsc-panel nsc-panel-compact nsc-hide">
    <div class="nsc-panel-move"></div>
    <div class="nsc-panel-tooltip">
        <div class="nsc-panel-tooltip-layout" layout="row" layout-align="start center">CTRL+V to toggle the panel</div>
    </div>

    <div class="nsc-panel-layout" flex="" layout="row" layout-align="start center">
        <div class="nsc-panel-groups" flex="" layout="row" layout-align="start start">

            <!-- group -->
            <div class="nsc-panel-group" flex="none" layout="row" layout-align="start start">
                <div class="nsc-panel-button separated active">
                    <div class="nsc-panel-select" flex="" layout="row">
                        <div class="nsc-panel-text nsc-noselect" flex="" layout="row" layout-align="center center">
                            <span class="nsc-icon nsc-icon-cursor-normal" data-i18n="videoPanelSimpleCursor" data-event="nimbus-editor-active-tools" data-event-param="cursorRing">&nbsp;</span>
                        </div>
                        <div class="nsc-panel-trigger">
                            <span class="nsc-icon nsc-icon-arrow">&nbsp;</span>
                        </div>
                    </div>
                    <div class="nsc-panel-dropdown to-top">
                        <ul flex="" layout="row" layout-align="start center">
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-cursor-shade" data-i18n="videoPanelFocusMouse" data-event="nimbus-editor-active-tools" data-event-param="cursorShadow">&nbsp;</span>
                            </li>
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-cursor-circle" data-i18n="videoPanelAnimatedCursor" data-event="nimbus-editor-active-tools" data-event-param="cursorRing">&nbsp;</span>
                            </li>
                            <!--<li class="nsc-panel-dropdown-icon " flex layout="row" layout-align="start center">-->
                            <!--<span class="nsc-icon nsc-icon-cursor-tail"></span>-->
                            <!--</li>-->
                            <!--<li class="nsc-panel-dropdown-icon " flex layout="row" layout-align="start center">-->
                            <!--<span class="nsc-icon nsc-icon-cursor-long"></span>-->
                            <!--</li>-->
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-cursor-normal" data-i18n="videoPanelSimpleCursor" data-event="nimbus-editor-active-tools" data-event-param="cursorDefault">&nbsp;</span>
                            </li>
                            <!--<li class="nsc-panel-dropdown-icon" flex layout="row" layout-align="start center">-->
                            <!--<span class="nsc-icon nsc-icon-cursor-none" data-event="nimbus-editor-active-tools" data-event-param="cursorNone"></span>-->
                            <!--</li>-->
                        </ul>
                    </div>
                </div>
            </div>
            <!-- /group -->

            <!-- group -->
            <div class="nsc-panel-group" flex="none" layout="row" layout-align="start start">
                <button class="nsc-panel-button" type="button">
                    <span class="nsc-icon nsc-icon-pen" data-i18n="videoPanelPen" data-event="nimbus-editor-active-tools" data-event-param="pen">&nbsp;</span>
                </button>
                <button class="nsc-panel-button" type="button">
                    <span class="nsc-icon nsc-icon-arrow-line" data-i18n="videoPanelArrow" data-event="nimbus-editor-active-tools" data-event-param="arrow">&nbsp;</span>
                </button>
                <button class="nsc-panel-button" type="button">
                    <span class="nsc-icon nsc-icon-square" data-i18n="videoPanelSquare" data-event="nimbus-editor-active-tools" data-event-param="square">&nbsp;</span>
                </button>
                <div class="nsc-panel-button separated">
                    <div class="nsc-panel-select" flex="" layout="row">
                        <div class="nsc-panel-text nsc-noselect" flex="" layout="row" layout-align="center center">
                            <span class="nsc-icon nsc-icon-attention" data-i18n="videoPanelMark" data-event="nimbus-editor-active-tools" data-event-param="notifRed">&nbsp;</span>
                        </div>
                        <div class="nsc-panel-trigger">
                            <span class="nsc-icon nsc-icon-arrow">&nbsp;</span>
                        </div>
                    </div>
                    <div class="nsc-panel-dropdown to-top">
                        <ul flex="" layout="row" layout-align="start center">
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-attention" data-i18n="videoPanelMark" data-event="nimbus-editor-active-tools" data-event-param="notifRed">&nbsp;</span>
                            </li>
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-question" data-i18n="videoPanelQuestion" data-event="nimbus-editor-active-tools" data-event-param="notifBlue">&nbsp;</span>
                            </li>
                            <li class="nsc-panel-dropdown-icon" flex="" layout="row" layout-align="start center">
                                <span class="nsc-icon nsc-icon-done" data-i18n="videoPanelCheckmark" data-event="nimbus-editor-active-tools" data-event-param="notifGreen">&nbsp;</span>
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="nsc-panel-button assembled">
                    <div class="nsc-panel-select" flex="" layout="row">
                        <div class="nsc-panel-text nsc-noselect" flex="" layout="row" layout-align="center center">
							<span class="nsc-icon nsc-icon-fill-none nsc-panel-icon-fill">
								<span class="nsc-panel-icon-fill-inner" id="nsc_panel_button_colors" style="background:#00FF00;" data-event="nimbus-editor-active-color" data-event-param="#00FF00">&nbsp;</span>
							</span>
                        </div>
                        <div class="nsc-panel-trigger">
                            <span class="nsc-icon nsc-icon-arrow">&nbsp;</span>
                        </div>
                    </div>
                    <div class="nsc-panel-dropdown">
                        <div class="nsc-panel-drop-area">
                            <div class="nsc-panel-colors">

                                <!-- picked -->
                                <div class="nsc-colors-picked">
                                    <div class="nsc-colors-picked-layout" flex="" layout="row" layout-align="start start" layout-wrap="">
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#000000">
                                                <span class="nsc-colors-picked-button-inner" style="background: #000000;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#0000FF">
                                                <span class="nsc-colors-picked-button-inner" style="background: #0000FF;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#FF00FF">
                                                <span class="nsc-colors-picked-button-inner" style="background: #FF00FF;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#00FFFF">
                                                <span class="nsc-colors-picked-button-inner" style="background: #00FFFF;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#00FF00">
                                                <span class="nsc-colors-picked-button-inner" style="background: #00FF00;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#FFFF00">
                                                <span class="nsc-colors-picked-button-inner" style="background: #FFFF00;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#FF0000">
                                                <span class="nsc-colors-picked-button-inner" style="background: #FF0000;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <div class="nsc-colors-picked-item">
                                            <button class="nsc-colors-picked-button" type="button" data-event="nimbus-editor-active-color" data-event-param="#FFFFFF">
                                                <span class="nsc-colors-picked-button-inner" style="background: #FFFFFF;">&nbsp;</span>
                                            </button>
                                        </div>
                                        <!--<div class="nsc-colors-picked-item">-->
                                        <!--<button class="nsc-colors-picked-button custom" type="button"> -->
                                        <!--<i class="nsc-icon ic-color-custom"></i> -->
                                        <!--</button>-->
                                        <!--</div>-->
                                    </div>
                                </div>
                                <!-- /picked -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- /group -->

            <!-- group -->
            <div class="nsc-panel-group" flex="none" layout="row" layout-align="start start">
                <button class="nsc-panel-button nsc-hide" type="button">
                    <span class="nsc-icon nsc-icon-eraser" data-i18n="videoPanelClear" data-event="nimbus-editor-active-tools" data-event-param="clear">&nbsp;</span>
                </button>
                <button class="nsc-panel-button" type="button">
                    <span class="nsc-icon nsc-icon-eraser-all" data-i18n="videoPanelClearAll" data-event="nimbus-editor-active-tools" data-event-param="clearAll">&nbsp;</span>
                </button>
                <button class="nsc-panel-button" type="button">
                    <span class="nsc-icon nsc-icon-webcam" data-i18n="videoPanelCamera" id="nimbus_web_camera_toggle">&nbsp;</span>
                </button>
            </div>
            <!-- /group -->
        </div>

        <div class="nsc-panel-actions" flex="none" layout="row" layout-align="start center">
            <button class="nsc-panel-button big" type="button" id="nsc_panel_button_play" style="display: none;">
                <span class="nsc-icon nsc-icon-play">&nbsp;</span>
            </button>
            <button class="nsc-panel-button big" type="button" id="nsc_panel_button_pause">
                <span class="nsc-icon nsc-icon-pause">&nbsp;</span>
            </button>
            <button class="nsc-panel-button big" type="button" id="nsc_panel_button_stop">
                <span class="nsc-icon nsc-icon-stop">&nbsp;</span>
            </button>
        </div>

        <!-- panel togglers -->
        <div class="nsc-panel-togglers" layout="row" layout-align="start center" flex="none">
            <button class="nsc-panel-toggle-button" type="button">
                <span class="nsc-icon nsc-icon-panel-close" data-i18n="videoPanelHideShowPanel">&nbsp;</span>
            </button>
        </div>
        <!-- /panel togglers -->

    </div>
</div>
<div class="nsc-video-editor nsc-hide events" style="width: 1263px; height: 15077px;"><canvas width="1263" height="15077" style="width: 1263px; height: 15077px; position: absolute; top: 0px; left: 0px; z-index: 0;"></canvas><canvas width="1263" height="15077" style="width: 1263px; height: 15077px; position: absolute; top: 0px; left: 0px; z-index: 1;"></canvas><canvas width="1263" height="15077" style="width: 1263px; height: 15077px; position: absolute; top: 0px; left: 0px; z-index: 2;"></canvas></div><div class="nsc-content-camera nsc-hide">
    <div class="nsc-content-camera-buttons" flex="none" layout="row" layout-align="start start">
        <button class="nsc-content-camera-button" type="button" id="nsc_video_camera_collapse" style="display: none">
            <span class="nsc-icon nsc-icon-panel-collapse"></span>
        </button>
        <button class="nsc-content-camera-button" type="button" id="nsc_video_camera_expand">
            <span class="nsc-icon nsc-icon-panel-expand"></span>
        </button>
        <button class="nsc-content-camera-button" type="button" id="nsc_video_camera_close">
            <span class="nsc-icon nsc-icon-panel-close"></span>
        </button>
    </div>
    <div class="nsc-content-camera-container">
        <div class="nsc-content-camera-shadow"></div>
    </div>
</div>
</body></html>